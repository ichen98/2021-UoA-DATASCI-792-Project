---
title: "Auckland Rugby Project - Trend Analysis of GPS Data"
author: "Ian Chen"
date: "30/07/2021"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r tidyverse, cache=TRUE, message=FALSE, warning=FALSE}
library(tidyverse)
```

First, I load the datasets in. The 2018 data notably has many more variables than the other years' data. These additional variables may be of use for the analysis, but because there is less data on these variables, they may necessitate a separate model that trains on only the 2018 data.

```{r loading the datasets, cache=TRUE}
# Loading the 2018 .csv files in
master2018 <- 
  list.files(path = "./2018_csvs/", pattern = "*.CSV", full.names = T) %>% 
  map_df(~read.csv(., skip = 4, header = TRUE))

master2019 <- 
  list.files(path = "./2019_csvs/", pattern = "*.CSV", full.names = T) %>% 
  map_df(~read.csv(., skip = 4, header = TRUE, colClasses = rep("character", 17)))

master2020 <- 
  list.files(path = "./2020_csvs/", pattern = "*.CSV", full.names = T) %>% 
  map_df(~read.csv(., skip = 4, header = TRUE))
# Getting the variable classes
varclasses <- unlist(lapply(master2018, class))
```

I begin with cleaning the 2018 .csv files.

The column names have varying degrees of spacing before and after words. They are cleaned up to have consistent names in proper English.

```{r cleaning up column names, cache=TRUE}
# Cleaning up column names
correctColumnNames <- c("Athlete", 
                        "Team", 
                        "Date", 
                        "Start Time", 
                        "Duration Total (s)", 
                        "Duration Speed Hi-Inten (s)", 
                        "Duration HR Hi-Inten (s)", 
                        "Distance Total (m)", 
                        "Distance Rate (m/min)", 
                        "Distance Speed Hi-Inten (m)", 
                        "Distance HR Hi-Inten (m)", 
                        "Speed Max (km/h)", 
                        "Sprints Total (num)", 
                        "Sprints Hi-Inten (num)", 
                        "Sprints HR Hi-Inten (num)", 
                        "HR Max Total (bpm)", 
                        "% Max HR", 
                        "Work Recovery Ratio", 
                        "Speed Duration Total (s)", 
                        "HR Duration Total (s)", 
                        "Athlete Load", 
                        "Metabolic PowerPeak", 
                        "Hi Int Acceleration (num)", 
                        "Hi Int Deceleration (num)", 
                        "Impact Rate (imp/min)", 
                        "Body Impacts (num)", 
                        "Hi Intensity Effort (num)", 
                        "HIE Rate", 
                        "Distance Speed Zone 1 (m)", 
                        "Distance Speed Zone 2 (m)", 
                        "Distance Speed Zone 3 (m)", 
                        "Distance Speed Zone 4 (m)", 
                        "Distance Speed Zone 5 (m)", 
                        "Sprints Speed Zone 3 (num)", 
                        "Sprints Speed Zone 4 (num)", 
                        "Sprints Speed Zone 5 (num)", 
                        "Duration HR Zone 4 (s)", 
                        "Duration HR Zone 5 (s)", 
                        "Accelerations Zone 3 (num)", 
                        "Accelerations Zone 4 (num)", 
                        "Accelerations Zone 5 (num)", 
                        "Decelerations Zone 3 (num)", 
                        "Decelerations Zone 4 (num)", 
                        "Decelerations Zone 5 (num)", 
                        "Body Impacts in Body Impacts Zone Total (num)",
                        "Body Impacts Grade 1 (num)", 
                        "Body Impacts Grade 2 (num)", 
                        "Body Impacts Grade 3 (num)", 
                        "Body Impacts Grade 4 (num)", 
                        "Body Impacts Grade 5 (num)")
colnames(master2018) <- correctColumnNames
```

Each individual .csv includes four opening rows that do not provide any meaningful information (which are skipped when the .csv is read into R), and three rows at the end that provide details about the average (mean), maximum and minimum values for each column. These are not useful for this analysis, so they should be removed here.

Furthermore, there are many data points that have missing data (represented by two asterisks - "**"). These need to be converted into NA values, which are easier to work around than a string of two asterisks forcing numeric columns into character columns.

```{r removing excess rows and converting missing values, cache=TRUE}
# Removing excess rows
master2018 <- subset(master2018, Athlete != "Avg" & Athlete != "Highest" & Athlete != "Lowest")

# Replacing all missing values with NA
master2018 <- na_if(master2018, "**")
```

The cells that were initially occupied by "**" strings forcibly converted their respective columns into character columns during the dataset import. These columns need to be converted into their proper class such that they can be useful for modeling.

First, the dates are imported into R as characters. For ease of reading, the data frame is sorted by date, from earliest to latest. This involves the conversion of the `Date` column into Date class objects, which requires all values in the `Date` column to be of a certain format.

The last column appears to be an error, not existing in the actual .csv files, so it is additionally dropped.

```{r conversion into proper classes, cache=TRUE}
# Converting dates into something usable
library(lubridate)
master2018$Date[21:38] <- "27/10/2018"
master2018$Date <- parse_date_time(master2018$Date, c("%d/%m/%Y"))
# Sorting by date, dropping redundant column `X`
master2018 <- master2018[order(as.Date(master2018$Date)), -51]
```

Columns 11 and 15 (`Distance HR Hi-Inten (m)` and `Sprints HR Hi-Inten (num)` respectively) are numeric values that were also imported into R as characters. These are transformed back into numeric variables. This is necessary for the proportional standardisation that is applied later.

```{r conversion into proper classes 2, cache=TRUE}
# Converting columns 11 and 15 back into numeric vectors
for (i in c(11, 15)) {
  master2018[, i] <- as.numeric(master2018[, i])
}
```

All durations are imported into R as character strings, as R can't parse the "MM:SS" format. Some preprocessing will need to be done with the times in the dataset, and by converting them into numeric values, manipulation of them will become a lot simpler. Therefore, all times in the data are converted to numeric values.

In this case, converting them to seconds is an easy way of standardising all of the times, making them integers. Integers make things easy to calculate without having to deal with fractions of a minute (which are in base 60).

```{r converting the times to seconds, cache=TRUE}
minsec_to_sec <- function(strvec) {
  # All durations are in "MM:SS" format; durations > 1 hr simply have MM > 59
  prelength <- ifelse(nchar(strvec) == 6, 3, ifelse(nchar(strvec) == 5, 2, 1))
  pre <- as.numeric(substr(strvec, 1, prelength))
  suf <- as.numeric(substr(strvec, nchar(strvec) - 1, nchar(strvec)))
  strvec <- pre * 60 + suf
  return(strvec)
}
master2018[, c(5:7, 19:20, 37:38)] <- lapply(master2018[, c(5:7, 19:20, 37:38)], minsec_to_sec)
```

A rugby union match goes for two 40-minute halves, with a halftime of a maximum length of 15 minutes. This sets a match at roughly a maximum of 95 minutes long. In the Mitre 10 Cup, should a semi-final or final match be tied at the end of regulation time, two 10-minute halves of extra time are played. This is the longest extension a Mitre 10 Cup game can have. Because much of the data's time values are abnormally high, a hard limit is set at 95 minutes (roughly the length of a regular match, including halftime), with the exception of the 2018 final, which went to extra time (resulting in a total of 120 minutes being played, so a hard limit of 120 minutes will be applied exclusively for that match).

95 minutes is equal to $95 \times 60 = 5700$ seconds, while 120 minutes is equal to $120 \times 60 = 7200$ seconds, so 5700 and 7200 will be the hard limits imposed on the minutes played.

Other duration variables may also have abnormally high values, so they will need to be adjusted too. These anomalous values are likely due to errors with the time tracking device, as it appears that many of the duration values are problematic.

If a player's total minutes played is cut down to the set ceiling, then the other duration variables are adjusted by calculating a proportion of the original minutes played, and using this proportion as a multiplier for the other duration variables. For instance, if a player has 100 minutes (6000 seconds) played in a non-2018-final match, that player's corresponding proportion is $5700/6000 = 0.95$, which then multiplies by the player's other duration values to give their adjusted values.

```{r adjusting the duration values, cache=TRUE}
# Calculating proportion by the above method
master2018$Proportion <- ifelse(
  as.character(master2018[, 3]) == "2018-10-27", 
  7200 / master2018$`Duration Total (s)`, 
  5700 / master2018$`Duration Total (s)`)
# Only interested in adjusting values that have a `Proportion` value < 1
master2018$Proportion[which(master2018$Proportion > 1)] <- 1
for (j in c(5, 7:8, 10:11, 13:15, 19:24, 26:27, 29:50)) {
  master2018[, j] <- master2018[, j] * master2018$Proportion
}
```

Column 17, `% Max HR`, contains a percentage symbol in each of the values. Because all of these values should be numeric, the percentage symbol is removed and `% Max HR` is converted to numeric.

```{r removing percentage symbols, cache=TRUE}
# Removing percentage symbols
master2018[, 17] <- as.numeric(substr(master2018[, 17], 1, nchar(master2018[, 17]) - 1))
```

Column 18, `Work Recovery Ratio`, contains a small set of unique values. This can be recoded into a factor.

```{r recoding work recovery ratio, cache=TRUE}
# Recoding Work Recovery Ratio into a factor
master2018[, 18] <- as.factor(master2018[, 18])
```

Player names are misspelled in different ways across each dataset. These must be standardised to allow for simpler merging of additional information.

```{r standardising names, cache=TRUE}
# Every name from every dataset combined
currentNames <- sort(unique(c(master2018$Athlete, master2019$Athlete, master2020$Athlete)))

# The incorrectly-recorded names
problematicNames <- c("Able, Rob", 
                      "Hallem Ewes, Liam", 
                      "Hodgmen, Alex", 
                      "Lemalu, Faatungu", 
                      "Liaana, Desma", 
                      "Liana, Desma", 
                      "Lundenmuth, Ezeikeil", 
                      "Reidler Kapa, Waimana", 
                      "Ruru, Jonathon", 
                      "Schwenke, Lief", 
                      "Scraffton, Scott", 
                      "Sosene, Mike", 
                      "Sotutu, Hoksins")
# The corrections to the above names
correctedNames <- c("Abel, Robbie", 
                    "Hallam-Eames, Liam", 
                    "Hodgman, Alex", 
                    "Lemalu, Fa'atiga", 
                    "Liaina, Desma", 
                    "Liaina, Desma", 
                    "Lindenmuth, Ezi", 
                    "Riedlinger-Kapa, Waimana", 
                    "Ruru, Jonathan", 
                    "Schwenke, Leif", 
                    "Scrafton, Scott", 
                    "Sosene-Feagai, Mike", 
                    "Sotutu, Hoskins")

# A function for name correction
nameCorrection <- function(data) {
  for (k in 1:length(problematicNames)) {
    data[which(data[, 1] == problematicNames[k]), 1] <- correctedNames[k]
  }
  return(data)
}

# Applying the function
master2018 <- nameCorrection(master2018)
```

Win margins will be used as a one-size-fits-all metric for measuring how good a player's performance in a match is i.e. the response variable for any fitted model. This is added to the main dataset.

```{r match win margin data, cache=TRUE}
# Dates of matches
matchDates <- as.Date(c("2018-08-18", 
                        "2018-08-26", 
                        "2018-08-30", 
                        "2018-09-07", 
                        "2018-09-16", 
                        "2018-09-22", 
                        "2018-09-28", 
                        "2018-10-04", 
                        "2018-10-10", 
                        "2018-10-14", 
                        "2018-10-20", 
                        "2018-10-27", 
                        "2019-08-09", 
                        "2019-08-15", 
                        "2019-08-24", 
                        "2019-08-31", 
                        "2019-09-08", 
                        "2019-09-14", 
                        "2019-09-22", 
                        "2019-09-27", 
                        "2019-10-05", 
                        "2019-10-11", 
                        "2019-10-19", 
                        "2020-09-12", 
                        "2020-09-20", 
                        "2020-09-27", 
                        "2020-10-02", 
                        "2020-10-10", 
                        "2020-10-17", 
                        "2020-10-24", 
                        "2020-10-31", 
                        "2020-11-07", 
                        "2020-11-15", 
                        "2020-11-21", 
                        "2020-11-28"))
# Match win margins by date
margins <- c(4, 16, 18, 26, 5, 1, -5, 5, 48, 16, 21, 7, 
             0, 33, 6, 0, -10, 15, -19, -40, 57, 24, -9, 
             32, -18, 38, 4, 1, 21, -1, 21, 4, -1, 5, -1)
# Combining date and win margins into one dataframe
winMargins <- data.frame(Date = matchDates, margins)
# Combining win margins into the main dataframe, merging by Date
master2018 <- left_join(master2018, winMargins)
```

I created two supplementary files to provide additional necessary variables. The first is `positional_data_by_match.csv`, which contains each match's game day squad. This provides the position that each player named in the squad for that matchup played at. The replacements (wearing jerseys 16-23) were labelled as 16, as the replacement jersey number does not provide exact positional information.

The second supplementary file is `positional data.csv`, which contains the preferred position for each player. This was determined by selecting the position in the starting XV that they appeared in the most over the matches represented in the dataset. For those that did not make any appearances in the starting XV, some Googling and some clarification with Paul Downes, my Auckland Rugby representative filled in their preferred position.

The positional data in `positional_data_by_match.csv` is added to the master dataset for the players that were named in the starting XV for each of the matches played in 2018. The preferred positions in `positional data.csv` is added to the master dataset for any players that were named as replacements, to show what position they would typically fill in if they had started the match with the starting XV.

```{r positional data by match, cache=TRUE}
# Positional data by match
matchPos <- read.csv("positional_data_by_match.csv", skip = 4)
# Rename columns to be consistent with the data
colnames(matchPos) <- c("Athlete", as.character(matchDates))
matchPos <- nameCorrection(matchPos)
# Initialise the position column
master2018$Position <- 0
# Go through each match day
for (l in 2:36) {
  currentDate <- colnames(matchPos)[l]
  # Get the squad that played on/was named for this day
  activeSquad <- matchPos[which(matchPos[, l] != 0), c(1, l)]
  playersOnThisDate <- which(master2018$Date == as.Date(currentDate) & master2018$Athlete %in% activeSquad[, 1])
  # Add the position for each player
  for (m in playersOnThisDate) {
    player <- which(activeSquad[, 1] == master2018[m, 1])
    master2018[m, 53] <- activeSquad[player, 2]
  }
}

# Now to deal with the replacements, which are all labelled 16
# The preferred positions are in "positional data.csv"
preferredPos <- read.csv("positional data.csv")[1:65, ]
colnames(preferredPos) <- c("Name", 
                            "1 - Loosehead prop", 
                            "2 - Hooker", 
                            "3 - Tighthead prop", 
                            "4 - Left lock", 
                            "5 - Right lock", 
                            "6 - Blindside flanker", 
                            "7 - Openside flanker", 
                            "8 - Number 8", 
                            "9 - Scrum-half", 
                            "10 - Fly-half", 
                            "11 - Left wing", 
                            "12 - Inside centre", 
                            "13 - Outside centre", 
                            "14 - Right wing", 
                            "15 - Fullback", 
                            "16-23 - Replacement", 
                            "Pref. pos. (number)", 
                            "Pref. pos. (text)", 
                            "Pref. group")
preferredPos <- nameCorrection(preferredPos)
# Coercing the columns of the preferred positional data into ideal classes
for (n in 2:18) {
  preferredPos[, n] <- as.numeric(preferredPos[, n])
}
# Getting the rows that correspond to replacements
replacements <- which(master2018$Position == 16)
# Giving the replacements their preferred position
for (o in replacements) {
  replacementName <- master2018[o, 1]
  master2018[o, 53] <- as.numeric(preferredPos[which(preferredPos[, 1] == replacementName), 18])
}
# Converting the positional data into a factor
master2018[, 53] <- as.factor(master2018[, 53])
```

Finally, excess rows are removed from the master dataset. These include rows where, for a given match, a player is listed multiple times, as well as players who are in the data but were not named to the 23-man match-day squad for that match. For the latter, they did not contribute to the win margin that corresponds to that match, so their data is not useful for prediction.

```{r removing excess rows, cache=TRUE}
# Removing duplicate rows
for (p in unique(master2018$Date)) {
  matchPlayers <- master2018[which(master2018$Date == p), 1]
  for (q in matchPlayers[duplicated(matchPlayers)]) {
    dupes <- master2018[which(master2018$Date == p & master2018$Athlete == q),]
    notHighestMinutes <- as.numeric(rownames(dupes[which(dupes$`Duration Total (s)` != max(dupes$`Duration Total (s)`)), ]))
    master2018 <- master2018[-notHighestMinutes,]
  }
}
# Deleting players that didn't play in the game, since there would be no win margin associated with their stats
master2018 <- master2018[-which(master2018$Position == 0),]
```

That should be all the preliminary cleaning and preprocessing that needs to be done. The same methods are applied to the 2019 data, although in a modified manner. The 2019 data has a far smaller subset of the variables in the 2018 data, so the column names here are different than the 2018 columns. Otherwise, the same cleaning is applied to the 2019 data, to keep the data consistent across each year.

```{r 2019, cache=TRUE}
# 2019 and 2020 datasets have fewer variables
colnames2019_20 <- c("Athlete", 
                    "Team", 
                    "Date", 
                    "Start Time", 
                    "Duration Total (s)", 
                    "Distance Total (m)", 
                    "Speed Max (km/h)", 
                    "Hi Int Acceleration (num)", 
                    "Distance Speed Zone 1 (m)", 
                    "Distance Speed Zone 2 (m)", 
                    "Distance Speed Zone 3 (m)", 
                    "Distance Speed Zone 4 (m)", 
                    "Distance Speed Zone 5 (m)", 
                    "Body Impacts in Body Impacts Zone Total (num)", 
                    "Sprints Speed Zone 3 (num)", 
                    "Sprints Speed Zone 4 (num)", 
                    "Sprints Speed Zone 5 (num)")
colnames(master2019) <- colnames2019_20

master2019 <- subset(master2019, Athlete != "Avg" & Athlete != "Highest" & Athlete != "Lowest")
master2019 <- na_if(master2019, "**")

master2019$Date <- parse_date_time(master2019$Date, c("%d/%m/%Y"))
master2019 <- master2019[order(as.Date(master2019$Date)), -18]

for (i in 6:17) {
  master2019[, i] <- as.numeric(master2019[, i])
}

master2019[, 5] <- minsec_to_sec(master2019[, 5])

master2019$Proportion <- 5700 / master2019$`Duration Total (s)`
master2019$Proportion[which(master2019$Proportion > 1)] <- 1
for (j in c(5:6, 8:17)) {
  master2019[, j] <- master2019[, j] * master2019$Proportion
}

master2019 <- nameCorrection(master2019) %>% 
  left_join(winMargins)

master2019$Position <- 0
for (l in 2:36) {
  currentDate <- colnames(matchPos)[l]
  activeSquad <- matchPos[which(matchPos[, l] != 0), c(1, l)]
  playersOnThisDate <- which(master2019$Date == as.Date(currentDate) & master2019$Athlete %in% activeSquad[, 1])
  for (m in playersOnThisDate) {
    player <- which(activeSquad[, 1] == master2019[m, 1])
    master2019[m, 20] <- activeSquad[player, 2]
  }
}
replacements <- which(master2019$Position == 16)
for (o in replacements) {
  replacementName <- master2019[o, 1]
  master2019[o, 20] <- as.numeric(preferredPos[which(preferredPos[, 1] == replacementName), 18])
}
master2019[, 20] <- as.factor(master2019[, 20])

for (p in unique(master2019$Date)) {
  matchPlayers <- master2019[which(master2019$Date == p), 1]
  for (q in matchPlayers[duplicated(matchPlayers)]) {
    dupes <- master2019[which(master2019$Date == p & master2019$Athlete == q),]
    notHighestMinutes <- as.numeric(rownames(dupes[which(dupes$`Duration Total (s)` != max(dupes$`Duration Total (s)`)), ]))
    master2019 <- master2019[-notHighestMinutes,]
  }
}
master2019 <- master2019[-which(master2019$Position == 0),]
```

The same is done with the 2020 dataset, which closely resembles the 2019 dataset in structure and format, including the number and name of columns.

```{r 2020, cache=TRUE}
colnames(master2020) <- colnames2019_20

master2020 <- subset(master2020, Athlete != "Avg" & Athlete != "Highest" & Athlete != "Lowest")
master2020 <- na_if(master2020, "**")

master2020$Date <- parse_date_time(master2020$Date, c("%d/%m/%Y"))
master2020 <- master2020[order(as.Date(master2020$Date)), -18]

for (i in 6:17) {
  master2020[, i] <- as.numeric(master2020[, i])
}

master2020[, 5] <- minsec_to_sec(master2020[, 5])

master2020$Proportion <- 5700 / master2020$`Duration Total (s)`
master2020$Proportion[which(master2020$Proportion > 1)] <- 1
for (j in c(5:6, 8:17)) {
  master2020[, j] <- master2020[, j] * master2020$Proportion
}

master2020 <- nameCorrection(master2020) %>% 
  left_join(winMargins)

master2020$Position <- 0
for (l in 2:36) {
  currentDate <- colnames(matchPos)[l]
  activeSquad <- matchPos[which(matchPos[, l] != 0), c(1, l)]
  playersOnThisDate <- which(master2020$Date == as.Date(currentDate) & master2020$Athlete %in% activeSquad[, 1])
  for (m in playersOnThisDate) {
    player <- which(activeSquad[, 1] == master2020[m, 1])
    master2020[m, 20] <- activeSquad[player, 2]
  }
}
replacements <- which(master2020$Position == 16)
for (o in replacements) {
  replacementName <- master2020[o, 1]
  master2020[o, 20] <- as.numeric(preferredPos[which(preferredPos[, 1] == replacementName), 18])
}
master2020[, 20] <- as.factor(master2020[, 20])

for (p in unique(master2020$Date)) {
  matchPlayers <- master2020[which(master2020$Date == p), 1]
  for (q in matchPlayers[duplicated(matchPlayers)]) {
    dupes <- master2020[which(master2020$Date == p & master2020$Athlete == q),]
    notHighestMinutes <- as.numeric(rownames(dupes[which(dupes$`Duration Total (s)` != max(dupes$`Duration Total (s)`)), ]))
    master2020 <- master2020[-notHighestMinutes,]
  }
}
master2020 <- master2020[-which(master2020$Position == 0),]
```

The 2019 and 2020 datasets are combined, since they share the same columns. The 2018 dataset is joined with them also, but only at the columns that are shared with the 2019 and 2020 datasets.

```{r combining the datasets, cache=TRUE}
combinedData <- rbind(master2018[, c(1:5, 8, 12, 23, 29:36, 45, 51:53)], master2019, master2020)
```

Finally, it appears that `Speed Duration Total (s)` and `HR Duration Total (s)` are not needed, since they measure the total duration of data collected beginning when the GPS unit locks (and when heart rate is detected for `HR Duration Total (s)`.) These are extremely correlated with `Duration Total (s)`, so these can be safely removed.

Additionally, `Body Impacts in Body Impact Zones Total (num)` is equal to the `Body Impacts (num)` measure in the 2018 dataset, appearing to capture the same information. Because the former is in all three datasets, the latter is removed, and the former is renamed to the simpler `Body Impacts (num)`.

```{r removing some duration variables, cache=TRUE}
master2018 <- master2018[, -c(19, 20, 26)]
colnames(master2018)[42] <- "Body Impacts (num)"
colnames(combinedData)[17] <- "Body Impacts (num)"
```

# Exploring the data

I want to explore the data to see if there are any interesting relationships between positional groups for each of the variables present in the datasets.

First, the variables unique to the 2018 dataset are plotted.

```{r exploring data 2018, cache=TRUE}
# 2018 dataset-unique variable visualisation
for (u in c(6:7, 9:11, 13:17, 19:20, 22:25, 34:41, 43:47)) {
  print(ggplot(master2018, aes(Position, master2018[, u])) + 
          geom_boxplot() + 
          geom_point(alpha = 0.3) + 
          ylab(colnames(master2018)[u]) + 
          geom_jitter())
}
```

There are a lot of plots here, but the main points I gleaned from these were that:

<ul>
  <li>Backs have high values for most sprint, distance and deceleration stats;</li>
  <li>Forwards tend to have high values for low grade body impacts, and low values for energy related measures;</li>
  <li>Heart rate threshold-related measures (e.g. `Duration HR Hi-Inten (s)`, `Distance HR Hi-Inten (m)`, `Sprints HR Hi-Inten (num)`) do not have any particular trend outside of inside centre (#12) and fullback (#15) registering high values;</li>
  <li>There are some observations for the heart rate measures `HR Max Total (bpm)` and `% Max HR` that are zeroes, which are unrealistic (as this would imply the players died - more likely to be due to a malfunction in the GPS unit);</li>
  <li>Scrum-half (#9) has a much higher high-intensity sprint number than every other position;</li>
  <li>Inside centre (#12) registers particularly high values for accelerations, decelerations and body impacts, but also high values overall for most of the other variables.</li>
</ul>

Now, the remaining variables shared between the 2018, 2019 and 2020 datasets are plotted.

```{r exploring data all datasets, cache=TRUE}
# Plotting the other variables
for (v in 5:17) {
  print(ggplot(combinedData, aes(Position, combinedData[, v])) + 
          geom_boxplot() + 
          geom_point(alpha = 0.2) + 
          ylab(colnames(combinedData)[v]) + 
          geom_jitter())
}
```

These plots mostly reinforce what is in the 2018 dataset-unique variable plots, but additionally:

<ul>
  <li>Front rowers (#1, #2, #3) have lower minutes and distance values than the other positions;</li>
  <li>For `Body Impacts in Body Impacts Zone Total (num)`, inside centre (#12) has a high distribution centre relative to the other backs. Overall, 12 has the second-highest distribution centre, which also contains the data points with the highest values across all positions.</li>
</ul>

Regarding the heart rate issues, the distributions are all centred quite similarly across all positions, and for the most part are quite narrow. I am quite comfortable with removing these variables completely, as they do not appear to show any meaningful trend.

```{r removing heart rate variables, cache=TRUE}
master2018 <- master2018[, -c(16, 17)]
```

# Finding the positional maximum, minimum and mean data for each marker

The 2018 dataset contains 53 variables, 33 of which are not shared by the 2019 and 2020 datasets. Within the 20 variables that are shared, 7 of them are not performance markers; they are either redundant information, unique identifiers for each individual row, or variables I created during the cleaning process.

Therefore, the 2018 dataset must be separated by position, and then the positional maximum, minimum and mean can be found. These values are printed. Variable 18, `HIE Rate` is not numeric, so it is left out here. Any performance markers that are shared by the 2018, 2019 and 2020 datasets are then found and printed afterwards.

```{r positional 2018, cache=TRUE}
for (r in 1:15) {
  # Finding the positional minimum, mean and maximum for performance markers exclusive to the 2018 dataset
  positionalData <- master2018[which(master2018$Position == r),]
  cat(paste0("POSITION: ", as.character(r)), "\n")
  for (s in c(6:7, 9:11, 13:15, 17:18, 20:23, 32:39, 41:45)) {
    print(paste0("Variable ", as.character(s), " - ", colnames(positionalData)[s], 
                 " - MIN: ", min(positionalData[, s], na.rm = TRUE), 
                 " | MEAN: ", mean(positionalData[, s], na.rm = TRUE), 
                 " | MAX: ", max(positionalData[, s], na.rm = TRUE)))
  }
  cat("\n")
  # Finding the positional minimum, mean and maximum for performance markers shared between the 2018, 2019 and 2020 datasets
  positionalSmallData <- combinedData[which(combinedData$Position == r),]
  for (t in 5:17) {
    print(paste0("Variable ", as.character(t), " - ", colnames(positionalSmallData)[t], 
                 " - MIN: ", min(positionalSmallData[, t], na.rm = TRUE), 
                 " | MEAN: ", mean(positionalSmallData[, t], na.rm = TRUE), 
                 " | MAX: ", max(positionalSmallData[, t], na.rm = TRUE)))
  }
  cat("\n", "\n", "\n")
}
```

# Fitting models to find top variables by position

To find the top variables by position, models need to be fitted. The easiest way to do this is to fit a separate model for the data filtered by each position.

```{r models, cache=TRUE}
library(caret)
fullyCombined <- full_join(master2018, combinedData)
dim(fullyCombined)
fullyCombined$Position <- droplevels(fullyCombined$Position)
levels(fullyCombined$`Work Recovery Ratio`) <- c(levels(fullyCombined$`Work Recovery Ratio`), "Not Applicable")
fullyCombined[which(is.na(fullyCombined$`Work Recovery Ratio`)), 16] <- "Not Applicable"
fullyCombined$`Work Recovery Ratio` <- relevel(fullyCombined$`Work Recovery Ratio`, "Not Applicable")
fullyCombined$`Work Recovery Ratio` <- droplevels(fullyCombined$`Work Recovery Ratio`)
# head(fullyCombined)
fullyCombined <- fullyCombined[, -c(nearZeroVar(fullyCombined))]
dim(fullyCombined)

for (u in 1:15) {
  imputations <- preProcess(fullyCombined[which(fullyCombined$Position == u), ], method = "medianImpute")
  fullyCombined[which(fullyCombined$Position == u), ] <- predict(imputations, fullyCombined[which(fullyCombined$Position == u), ])
}

# imputations <- preProcess(fullyCombined, method = "medianImpute")
# data <- predict(imputations, fullyCombined)
```

## Backward stepwise selection

For backward stepwise selection, the datasets are split by position. A full model is fitted for each split to obtain the coefficients for the variables when all are taken into account. Some variables may result in singularities, which are most likely due to highly correlated variables coexisting in the dataset. By creating a correlation matrix with `cor()`, and finding variables with correlations beyond a certain cutoff using `findCorrelation()`, these variables can be singled out and removed from the data.

The top five variables by backward stepwise selection are then determined with `regsubsets(..., method = "backward")`. The most important variable is removed last, and as such is the only variable in the one-variable model. The second-most important variable is removed penultimately, and as such is the variable that differs between the one-variable and two-variable models, etc. The full model coefficients for these variables can then be determined and analysed.

The results are presented as an ordered list, from most important variable to fifth-most important variable. Next to each selected variable is its coefficient estimate and its corresponding p-value. Underneath each selected variable is at least one bullet point that provides in plain English an interpretation of the 95% confidence interval for the variable's coefficient estimate.

### Position 1: Loosehead prop

```{r backwards stepwise selection 1, cache=TRUE}
library(leaps)

pos1data <- fullyCombined[which(fullyCombined$Position == 1), -c(1:4)]
pos1data$`Work Recovery Ratio` <- droplevels(pos1data$`Work Recovery Ratio`)
pos1data <- pos1data[, -c(38, 40)]

corr1 <- cor(pos1data[, -c(11, 38)])
print("Which variables have high correlations with other variables?")
findCorrelation(corr1, cutoff = 0.999)
findCorrelation(corr1, cutoff = 0.99)
findCorrelation(corr1, cutoff = 0.95)
findCorrelation(corr1, cutoff = 0.9)
findCorrelation(corr1, cutoff = 0.85)
findCorrelation(corr1, cutoff = 0.8)
findCorrelation(corr1, cutoff = 0.75)
sort(findCorrelation(corr1, cutoff = 0.7))

# Removing variables that are causing singularities
pos1data <- pos1data[, -c(3, 4, 8, 10:11, 13:15, 17, 20, 23:24, 35)]

model1.1 <- regsubsets(margins ~ ., data = pos1data, method = "backward", nvmax = 100)
coef(model1.1, c(1:5))
full1.1 <- lm(margins ~ . , data = pos1data)
summary(full1.1)
confint(full1.1)[c(13, 12, 3, 20, 17), ]
```

These five models suggest that the most important GPS variables for a loosehead prop are, beginning from the most important:
<ol>
  <li>`Distance Speed Zone 5 (m)` | +31.6, p-value = 0.00415
    <ul>
      <li>Every additional metre a loosehead prop covers in Speed ZOne 5 contributes between +10.7 and +52.5 points to the win margin</li>
    </ul>
  </li>
  <li>`Distance Speed Zone 4 (m)` | -1.59, p-value = 0.06546
    <ul>
      <li>Every additional metre a loosehead prop covers in Speed Zone 4 contributes between -3.29 and +0.11 points to the win margin</li>
    </ul>
  </li>
  <li>`Duration Speed Hi-Inten (s)` | -40.8, p-value = 0.11055
    <ul>
      <li>Every additional second a loosehead prop manages to maintain a speed beyond the high intensity speed threshold contributes between -91.4 and +9.8 points to the win margin</li>
    </ul>
  </li>
  <li>`Decelerations Zone 3 (num)` | +26.6, p-value = 0.12212
    <ul>
      <li>Every additional deceleration a loosehead prop performs in Deceleration Zone 3 contributes between -7.5 and +60.6 points to the win margin</li>
    </ul>
  </li>
  <li>`Accelerations Zone 3 (num)` | -19.4, p-value = 0.01491
    <ul>
      <li>Every additional acceleration a loosehead prop performs in Acceleration Zone 3 contributes between -34.8 and -4.0 points to the win margin</li>
    </ul>
  </li>
</ol>

### Position 2: Hooker

```{r backwards stepwise selection 2, cache=TRUE}
pos2data <- fullyCombined[which(fullyCombined$Position == 2), -c(1:4)]
pos2data$`Work Recovery Ratio` <- droplevels(pos2data$`Work Recovery Ratio`)
# All zeroes for Decelerations Zone 5 (num)
pos2data <- pos2data[, -c(33, 38, 40)]

corr2 <- cor(pos2data[, -c(11, 38)])
print("Which variables have high correlations with other variables?")
findCorrelation(corr2, cutoff = 0.999)
findCorrelation(corr2, cutoff = 0.99)
findCorrelation(corr2, cutoff = 0.98)

# Removing variables that are causing singularities
pos2data <- pos2data[, -c(6, 10, 17)]

model1.2 <- regsubsets(margins ~ ., data = pos2data, method = "backward", nvmax = 100)
coef(model1.2, 1:5)
full1.2 <- lm(margins ~ ., data = pos2data)
summary(full1.2)
confint(full1.2)[c(18, 5, 17, 20, 19), ]
```

These five models suggest that the most important GPS variables for a hooker are, beginning from the most important:
<ol>
  <li>`Distance Speed Zone 2 (m)` | +7.37, p-value = 0.371
    <ul>
      <li>Every additional metre a hooker covers in Speed Zone 2 contributes between -9.24 and +23.98 points to the win margin</li>
    </ul>
  </li>
  <li>`Distance Total (m)` | -7.39, p-value = 0.371
    <ul>
      <li>Every additional metre a hooker covers in a match contributes between -24.04 and +9.25 points to the win margin</li>
    </ul>
  </li>
  <li>`Distance Speed Zone 1 (m)` | +7.40, p-value = 0.371
    <ul>
      <li>Every additional metre a hooker covers in Speed Zone 1 contributes between -9.25 and +24.05 points to the win margin</li>
    </ul>
  </li>
  <li>`Distance Speed Zone 4 (m)` | +8.37, p-value = 0.300
    <ul>
      <li>Every additional metre a hooker covers in Speed Zone 4 contributes between -7.87 and +24.61 points to the win margin</li>
    </ul>
  </li>
  <li>`Distance Speed Zone 3 (m)` | +7.47, p-value = 0.360
    <ul>
      <li>Every additional metre a hooker covers in Speed Zone 3 contributes between -8.99 and +23.94 points to the win margin</li>
    </ul>
  </li>
</ol>

### Position 3: Tighthead prop

```{r backwards stepwise selection 3, cache=TRUE}
pos3data <- fullyCombined[which(fullyCombined$Position == 3), -c(1:4)]
pos3data$`Work Recovery Ratio` <- droplevels(pos3data$`Work Recovery Ratio`)
# All zeroes for Sprints Speed Zone 5 (num), Decelerations Zones 4 and 5 (num)
pos3data <- pos3data[, -c(25, 32, 33, 38, 40)]

corr3 <- cor(pos3data[, -c(11, 38)])
print("Which variables have high correlations with other variables?")
findCorrelation(corr3, cutoff = 0.999)
findCorrelation(corr3, cutoff = 0.99)
findCorrelation(corr3, cutoff = 0.9)
findCorrelation(corr3, cutoff = 0.85)
findCorrelation(corr3, cutoff = 0.8)
findCorrelation(corr3, cutoff = 0.75)
findCorrelation(corr3, cutoff = 0.7)
findCorrelation(corr3, cutoff = 0.67)

# Removing variables that are causing singularities
pos3data <- pos3data[, -c(3, 4, 6, 8, 13:18, 20, 22, 25, 31)]

model1.3 <- regsubsets(margins ~ ., data = pos3data, method = "backward", nvmax = 100)
coef(model1.3, 1:5)
full1.3 <- lm(margins ~ ., data = pos3data)
summary(full1.3)
```

`Work Recovery Ratio` has two dummy variables represented in the five-variable model. We will look into larger models until a fifth non-dummy variable is found.

```{r backwards stepwise selection 3.1, cache=TRUE}
coef(model1.3, 6)
confint(full1.3)[c(7:9, 2, 18, 5), ]
```

These six models suggest that the most important GPS variables for a tighthead prop are, beginning from the most important:
<ol>
  <li>`Sprints HR Hi-Inten (num)` | +1.05, p-value = 0.394
    <ul>
      <li>Every additional sprint a tighthead prop performs while over the high intensity HR benchmark contributes between -1.45 and +3.55 points to the win margin</li>
    </ul>
  </li>
  <li>`Work Recovery Ratio` | 1:1 -> +369, p-value = 0.439; 2:3 -> +469, p-value = 0.434
    <ul>
      <li>If a tighthead prop has a Work Recovery Ratio of 1:1 in a match, it will contribute between -602 and +1340 points(!) to the win margin</li>
      <li>If a tighthead prop has a Work Recovery Ratio of 2:3 in a match, it will contribute between -751 and +1690 points(!) to the win margin</li>
    </ul>
  </li>
  <li>`Duration Total (s)` | -0.00228, p-value = 0.609
    <ul>
      <li>Every additional second a tighthead prop plays in a match contributes between -0.01137 and +0.00682 points to the win margin </li>
    </ul>
  </li>
  <li>`Accelerations Zone 5 (num)` | +97.0, p-value = 0.397
    <ul>
      <li>Every additional acceleration a tighthead prop performs in Acceleration Zone 5 contributes between -135.9 and +329.8 points(!) to the win margin</li>
    </ul>
  </li>
  <li>`Speed Max (km/h)` | -1.33, p-value = 0.542
    <ul>
      <li>For every kilometre per hour in a tighthead prop's maximum speed in a match, between -5.80 and +3.13 points are added to the win margin</li>
    </ul>
  </li>
</ol>

The tighthead prop data contains some surprisingly high coefficient magnitudes. This may be due to the number of variables removed from the dataset to deal with singularities in the full model. Even then, some singularities remain, but the correlation cutoff has reached close to 0.5, and the offending variables were not singled out for removal.

### Position 4: Left lock

```{r backwards stepwise selection 4, cache=TRUE}
pos4data <- fullyCombined[which(fullyCombined$Position == 4), -c(1:4)]
pos4data$`Work Recovery Ratio` <- droplevels(pos4data$`Work Recovery Ratio`)
# All zeroes in Duration Speed Hi-Inten (s)
pos4data <- pos4data[, -c(2, 38, 40)]

corr4 <- cor(pos4data[, -c(10, 37)])
print("Which variables have high correlations with other variables?")
findCorrelation(corr4, cutoff = 0.999)
findCorrelation(corr4, cutoff = 0.9)
findCorrelation(corr4, cutoff = 0.8)
findCorrelation(corr4, cutoff = 0.75)
sort(findCorrelation(corr4, cutoff = 0.7))

# Removing variables that are causing singularities
pos4data <- pos4data[, -c(1:2, 5:6, 8:10, 12:16, 19, 20, 26)]

model1.4 <- regsubsets(margins ~ ., data = pos4data, method = "backward", nvmax = 100)
coef(model1.4, 1:5)
full1.4 <- lm(margins ~ ., data = pos4data)
summary(full1.4)
confint(full1.4)[c(6, 2, 7, 5, 4), ]
```

These five models suggest that the most important GPS variables for a left lock are, beginning from the most important:
<ol>
  <li>`Distance Speed Zone 1 (m)` | +0.594, p-value = 0.0418
    <ul>
      <li>Every additional metre a left lock covers in Speed Zone 1 contributes between +0.025 and +1.163 to the win margin</li>
    </ul>
  </li>
  <li>`Distance Total (m)` | -0.591, p-value = 0.0441
    <ul>
      <li>Every additional metre a left lock covers in the match contributes between -1.164 and -0.018 points to the win margin</li>
    </ul>
  </li>
  <li>`Distance Speed Zone 2 (m)` | +0.643, p-value = 0.0728
    <ul>
      <li>Every additional metre a left lock covers in Speed Zone 2 contributes between -0.067 and +1.354 points to the win margin</li>
    </ul>
  </li>
  <li>`Athlete Load` | -5.56, p-value = 0.5395
    <ul>
      <li>Every additional point in Athlete Load a left lock has contributes between -24.43 and +13.31 points to the win margin</li>
    </ul>
  </li>
  <li>`Sprints Total (num)` | +1.91, p-value = 0.4862
    <ul>
      <li>Every additional sprint a left lock performs in the match contributes between -3.80 and +7.63 points to the win margin</li>
    </ul>
  </li>
</ol>

### Position 5: Right lock

```{r backwards stepwise selection 5, cache=TRUE}
pos5data <- fullyCombined[which(fullyCombined$Position == 5), -c(1:4)]
pos5data$`Work Recovery Ratio` <- droplevels(pos5data$`Work Recovery Ratio`)

pos5data <- pos5data[, -c(38, 40)]

corr5 <- cor(pos5data[, -c(11, 38)])
print("Which variables have high correlations with other variables?")
findCorrelation(corr5, cutoff = 0.999)
findCorrelation(corr5, cutoff = 0.9)
findCorrelation(corr5, cutoff = 0.8)
findCorrelation(corr5, cutoff = 0.75)

# Removing variables that are causing singularities
pos5data <- pos5data[, -c(1:2, 4:5, 8, 10:11, 13:15, 17:18, 21, 25:26, 28:30, 35)]

model1.5 <- regsubsets(margins ~ ., data = pos5data, method = "backward", nvmax = 100)
coef(model1.5, 1:5)
full1.5 <- lm(margins ~ ., data = pos5data)
summary(full1.5)
confint(full1.5)[c(5, 11, 14, 18, 7), ]
```

These five models suggest that the most important GPS variables for a right lock are, beginning from the most important:
<ol>
  <li>`Sprints Hi-Inten (num)` | +5.65, p-value = 0.143
    <ul>
      <li>Every additional sprint a right lock performs above the high intensity sprint benchmark contributes between -2.06 and +13.36 points to the win margin</li>
    </ul>
  </li>
  <li>`Sprints Speed Zone 3 (num)` | -5.97, p-value = 0.296
    <ul>
      <li>Every additional sprint a right lock performs in Speed Zone 3 contributes between -17.55 and +5.60 points to the win margin</li>
    </ul>
  </li>
  <li>`Decelerations Zone 3 (num)` | +163, p-value = 0.479
    <ul>
      <li>Every additional deceleration a right lock performs in Deceleration Zone 3 contributes between -306 and +633 points(!) to the win margin</li>
    </ul>
  </li>
  <li>`Body Impacts Grade 2 (num)` | +25.7, p-value = 0.110
    <ul>
      <li>Every additional Grade 2 body impact a right lock performs contributes between -6.3 and +57.8 points to the win margin</li>
    </ul>
  </li>
  <li>`Hi Intensity Effort (num)` | -0.678, p-value = 0.765
    <ul>
      <li>For every additional effort a right lock performs that falls under any of the five high intensity categories (Hi-Int Sprints, Hi-Int Accelerations, Hi-Int Decelerations, Body Impacts and Jumps), between -5.326 and +3.970 points are added to the win margin</li>
    </ul>
  </li>
</ol>

### Position 6: Blindside flanker

```{r backwards stepwise selection 6, cache=TRUE}
pos6data <- fullyCombined[which(fullyCombined$Position == 6), -c(1:4)]
pos6data$`Work Recovery Ratio` <- droplevels(pos6data$`Work Recovery Ratio`)
# All zeroes in Duration Speed Hi-Inten (s)
pos6data <- pos6data[, -c(2, 38, 40)]

corr6 <- cor(pos6data[, -c(10, 37)])
print("Which variables have high correlations with other variables?")
findCorrelation(corr6, cutoff = 0.999)
findCorrelation(corr6, cutoff = 0.92)
findCorrelation(corr6, cutoff = 0.9)

# Removing variables that are causing singularities
pos6data <- pos6data[, -c(3, 5, 7, 9, 12, 14, 16)]

model1.6 <- regsubsets(margins ~ ., data = pos6data, method = "backward", nvmax = 100)
coef(model1.6, 1:5)
full1.6 <- lm(margins ~ ., data = pos6data)
summary(full1.6)
confint(full1.6)[c(13, 18, 5, 26, 8), ]
```

These five models suggest that the most important GPS variables for a blindside flanker are, beginning from the most important:
<ol>
  <li>`Distance Speed Zone 2 (m)` | +0.263, p-value = 0.415
    <ul>
      <li>Every additional metre a blindside flanker covers in Speed Zone 2 contributes between -0.433 and +0.959 points to the win margin</li>
    </ul>
  </li>
  <li>`Sprints Speed Zone 4 (num)` | +18.8, p-value = 0.537
    <ul>
      <li>Every additional sprint a blindside flanker performs in Speed Zone 4 contributes between -47.5 and +85.1 points to the win margin</li>
    </ul>
  </li>
  <li>`Speed Max (km/h)` | -5.65, p-value = 0.608
    <ul>
      <li>For every kilometre per hour in a blindside flanker's maximum speed in a match, between -29.73 and +18.43 points are added to the win margin</li>
    </ul>
  </li>
  <li>`Decelerations Zone 4 (num)` | +31.7, p-value = 0.818
    <ul>
      <li>Every additional deceleration a blindside flanker performs in Deceleration Zone 4 contributes between -270.4 and +333.8 points(!) to the win margin</li>
    </ul>
  </li>
  <li>`Work Recovery Ratio` | 2:3 -> -62.4, p-value = 0.711
    <ul>
      <li>If a blindside flanker has a Work Recovery Ratio of 2:3 in a match, it will contribute between -431.3 and +306.6 points(!) to the win margin</li>
    </ul>
  </li>
</ol>

### Position 7: Openside flanker

```{r backwards stepwise selection 7, cache=TRUE}
pos7data <- fullyCombined[which(fullyCombined$Position == 7), -c(1:4)]
pos7data$`Work Recovery Ratio` <- droplevels(pos7data$`Work Recovery Ratio`)

pos7data <- pos7data[, -c(38, 40)]

corr7 <- cor(pos7data[, -c(11, 38)])
print("Which variables have high correlations with other variables?")
findCorrelation(corr7, cutoff = 0.999)
findCorrelation(corr7, cutoff = 0.9)
findCorrelation(corr7, cutoff = 0.85)
findCorrelation(corr7, cutoff = 0.8)
findCorrelation(corr7, cutoff = 0.77)

# Removing variables that are causing singularities
pos7data <- pos7data[, -c(2:6, 8, 10:11, 13, 15:18)]

model1.7 <- regsubsets(margins ~ ., data = pos7data, method = "backward", nvmax = 100)
coef(model1.7, 1:5)
full1.7 <- lm(margins ~ ., data = pos7data)
summary(full1.7)
confint(full1.7)[c(18, 12, 25, 14, 24), ]
```

These five models suggest that the most important GPS variables for an openside flanker are, beginning from the most important:
<ol>
  <li>`Accelerations Zone 5 (num)` | +1024, p-value = 0.2008
    <ul>
      <li>Every additional acceleration an openside flanker performs in Acceleration Zone 5 contributes between -617 and 2666 points(!) to the win margin</li>
    </ul>
  </li>
  <li>`Sprints Speed Zone 4 (num)` | -32.0, p-value = 0.0198
    <ul>
      <li>Every additional sprint an openside flanker performs in Speed Zone 4 contributes between -58.0 and -6.0 points(!) to the win margin</li>
    </ul>
  </li>
  <li>`Body Impacts Grade 3 (num)` | +344, p-value = 0.2431
    <ul>
      <li>Every additional Grade 3 body impact an openside flanker performs contributes between -264 and +951 points(!) to the win margin</li>
    </ul>
  </li>
  <li>`Duration HR Zone 4 (s)` | +0.467, p-value = 0.2945
    <ul>
      <li>Every additional second an openside flanker spends in HR Zone 4 contributes between -0.457 and +1.391 points to the win margin</li>
    </ul>
  </li>
  <li>`Body Impacts Grade 2 (num)` | -150, p-value = 0.2286
    <ul>
      <li>Every additional Grade 2 body impact an openside flanker performs contributes -408 and +107 points(!) to the win margin</li>
    </ul>
  </li>
</ol>

Like the tighthead prop model, there are coefficients here that are also very large in magnitude. Singularities have been removed here, so this could be due to some variables being mostly zero-valued, but with a very small portion of non-zero values that are associated with a large-magnitude win margin.

### Position 8: Number 8

```{r backward stepwise selection 8, cache=TRUE}
pos8data <- fullyCombined[which(fullyCombined$Position == 8), -c(1:4)]
pos8data$`Work Recovery Ratio` <- droplevels(pos8data$`Work Recovery Ratio`)
# All zeroes for Duration Speed Hi-Inten (s) and Body Impacts Grade 3 (num)
pos8data <- pos8data[, -c(2, 37, 38, 40)]

corr8 <- cor(pos8data[, -c(10, 36)])
print("Which variables have high correlations with other variables?")
findCorrelation(corr8, cutoff = 0.999)
findCorrelation(corr8, cutoff = 0.9)

# Removing variables that are causing singularities
pos8data <- pos8data[, -c(3, 5, 7, 14:16)]

model1.8 <- regsubsets(margins ~ ., data = pos8data, method = "backward", nvmax = 100)
coef(model1.8, 1:5)
full1.8 <- lm(margins ~ ., data = pos8data)
summary(full1.8)
confint(full1.8)[c(23, 14, 16, 20, 17), ]
```

These five models suggest that the most important GPS variables for a number 8 are, beginning from the most important:
<ol>
  <li>`Duration HR Zone 4 (s)` | +0.0498, p-value = 0.6050
    <ul>
      <li>Every additional second a number 8 spends in HR Zone 4 contributes between -0.1512 and +0.2508 points to the win margin</li>
    </ul>
  </li>
  <li>`Hi Int Acceleration (num)` | -0.425, p-value = 0.3578
    <ul>
      <li>Every additional accleration a number 8 performs over the high intensity acceleration benchmark contributes between -1.380 and +0.530 points to the win margin</li>
    </ul>
  </li>
  <li>`Distance Speed Zone 2 (m)` | +0.188, p-value = 0.0847
    <ul>
      <li>Every additional metre a number 8 covers in Speed Zone 2 contributes between -0.029 and +0.405 points to the win margin</li>
    </ul>
  </li>
  <li>`Sprints Speed Zone 3 (num)` | +18.5, p-value = 0.0470
    <ul>
      <li>Every additional sprint a number 8 performs in Speed Zone 3 contributes between +0.3 and +36.7 points to the win margin</li>
    </ul>
  </li>
  <li>`Distance Speed Zone 3 (m)` | -0.860, p-value = 0.0446
    <ul>
      <li>Every additional metre a number 8 covers in Speed Zone 3 contributes between -1.696 and -0.024 points to the win margin</li>
    </ul>
  </li>
</ol>

### Position 9: Scrum-half

```{r backward stepwise selection 9, cache=TRUE}
pos9data <- fullyCombined[which(fullyCombined$Position == 9), -c(1:4)]
pos9data$`Work Recovery Ratio` <- droplevels(pos9data$`Work Recovery Ratio`)
# All zeroes for Duration Speed Hi-Inten (s)
pos9data <- pos9data[, -c(2, 38, 40)]

corr9 <- cor(pos9data[, -c(10, 37)])
print("Which variables have high correlations with other variables?")
findCorrelation(corr9, cutoff = 0.999)
findCorrelation(corr9, cutoff = 0.9)

# Removing variables that are causing singularities
pos9data <- pos9data[, -c(3, 7, 9, 10, 16, 25)]

model1.9 <- regsubsets(margins ~ ., data = pos9data, method = "backward", nvmax = 100)
coef(model1.9, 1:5)
full1.9 <- lm(margins ~ ., data = pos9data)
summary(full1.9)
confint(full1.9)[c(26, 15, 18, 3, 27), ]
```

These five models suggest that the most important GPS variables for a scrum-half are, beginning from the most important:
<ol>
  <li>`Decelerations Zone 4 (num)` | +50.8, p-value = 0.0936
    <ul>
      <li>Every additional deceleration a scrum-half performs in Deceleration Zone 4 contributes between -9.3 and +110.8 points(!) to the win margin</li>
    </ul>
  </li>
  <li>`Distance Speed Zone 3 (m)` | +0.406, p-value = 0.0493
    <ul>
      <li>Every additional metre a scrum-half covers in Speed Zone 3 contributes between +0.001 and +0.810 points to the win margin</li>
    </ul>
  </li>
  <li>`Sprints Speed Zone 3 (num)` | -6.80, p-value = 0.0804
    <ul>
      <li>Every additional sprint a scrum-half performs in Speed Zone 3 contributes between -14.48 and +0.89 points to the win margin</li>
    </ul>
  </li>
  <li>`Duration HR Hi-Inten (s)` | +0.0758, p-value = 0.6968
    <ul>
      <li>Every additional second a scrum-half spends over the high intensity heart rate benchmark contributes between -0.3214 and +0.4730 points to the win margin</li>
    </ul>
  </li>
  <li>`Decelerations Zone 5 (num)` | +91.5, p-value = 0.5444
    <ul>
      <li>Every additional deceleration a scrum-half performs in Deceleration Zone 5 contributes between -216.2 and +399.2 points(!) to the win margin</li>
    </ul>
  </li>
</ol>

# Position 10: Fly-half

```{r backward stepwise selection 10, cache=TRUE}
pos10data <- fullyCombined[which(fullyCombined$Position == 10), -c(1:4)]
pos10data$`Work Recovery Ratio` <- droplevels(pos10data$`Work Recovery Ratio`)
# All zeroes for Duration Speed Hi-Inten (s) and Accelerations Zone 5 (num)
pos10data <- pos10data[, -c(2, 30, 38, 40)]

corr10 <- cor(pos10data[, -c(10, 36)])
print("Which variables have high correlations with other variables?")
findCorrelation(corr10, cutoff = 0.999)
findCorrelation(corr10, cutoff = 0.9)
findCorrelation(corr10, cutoff = 0.8)
findCorrelation(corr10, cutoff = 0.75)
findCorrelation(corr10, cutoff = 0.73)

# Removing variables that are causing singularities
pos10data <- pos10data[, -c(1:4, 7, 9, 10, 12:14, 16, 18, 20:23, 28)]

model1.10 <- regsubsets(margins ~ ., data = pos10data, method = "backward", nvmax = 100)
coef(model1.10, 1:5)
full1.10 <- lm(margins ~ ., data = pos10data)
summary(full1.10)
confint(full1.10)[c(4, 13, 14, 8, 16), ]
```

These five models suggest that the most important GPS variables for a fly-half are, beginning from the most important:
<ol>
  <li>`Sprints Hi-Inten (num)` | -67.2, p-value = 0.2797
    <ul>
      <li>Every additional sprint a fly-half performs above the high intensity sprint benchmark contributes between -192.0 and +57.7 points(!) to the win margin</li>
    </ul>
  </li>
  <li>`Decelerations Zone 3 (num)` | +37.6, p-value = 0.3443
    <ul>
      <li>Every additional deceleration a fly-half performs in Deceleration Zone 3 contributes between -42.6 and +117.9 points(!) to the win margin</li>
    </ul>
  </li>
  <li>`Decelerations Zone 4 (num)` | -48.4, p-value = 0.4099
    <ul>
      <li>Every additional deceleration a fly-half performs in Deceleration Zone 4 contributes between -167.2 and +70.3 points(!) to the win margin</li>
    </ul>
  </li>
  <li>`Distance Speed Zone 3 (m)` | +0.155, p-value = 0.0548
    <ul>
      <li>Every additional metre a fly-half covers in Speed Zone 3 contributes between -0.003 and +0.314 points to the win margin</li>
    </ul>
  </li>
  <li>`Body Impacts (num)` | 0.654, p-value = 0.5722
    <ul>
      <li>Every additional body impact a fly-half makes in a match contributes between -2.999 and +1.692 points to the win margin</li>
    </ul>
  </li>
</ol>

### Position 11: Left wing

```{r backward stepwise selection 11, cache=TRUE}
pos11data <- fullyCombined[which(fullyCombined$Position == 11), -c(1:4)]
pos11data$`Work Recovery Ratio` <- droplevels(pos11data$`Work Recovery Ratio`)

pos11data <- pos11data[, -c(38, 40)]

corr11 <- cor(pos11data[, -c(11, 38)])
print("Which variables have high correlations with other variables?")
findCorrelation(corr11, cutoff = 0.999)
findCorrelation(corr11, cutoff = 0.9)
findCorrelation(corr11, cutoff = 0.8)
findCorrelation(corr11, cutoff = 0.75)
findCorrelation(corr11, cutoff = 0.7)
findCorrelation(corr11, cutoff = 0.66)

# Removing variables that are causing singularities
pos11data <- pos11data[, -c(1, 3:6, 8:10, 13:17, 24, 26:28, 32)]

model1.11 <- regsubsets(margins ~ ., data = pos11data, method = "backward", nvmax = 100)
coef(model1.11, 1:5)
full1.11 <- lm(margins ~ ., data = pos11data)
summary(full1.11)
confint(full1.11)[c(9, 5, 16:17, 11), ]
```

These five models suggest that the most important GPS variables for a left wing are, beginning from the most important:
<ol>
  <li>`Distance Speed Zone 3 (m)` | +0.0497, p-value = 0.824
    <ul>
      <li>Every additional metre a left wing covers in Speed Zone 3 contributes between -0.4261 and +0.5254 points to the win margin</li>
    </ul>
  </li>
  <li>`Work Recovery Ratio` | 2:3 -> +29.0, p-value = 0.804
    <ul>
      <li>If a left wing has a Work Recovery Ratio of 2:3 in a match, it will contribute between -220.1 to +278.2 points(!) to the win margin</li>
    </ul>
  </li>
  <li>`Decelerations Zone 3` | +76.0, p-value = 0.603
    <ul>
      <li>Every additional deceleration a left wing performs in Deceleration Zone 3 contributes between -234.1 and +386.1 points(!) to the win margin</li>
    </ul>
  </li>
  <li>`Decelerations Zone 5` | -35.1, p-value = 0.570
    <ul>
      <li>Every additional deceleration a left wing performs in Deceleration Zone 5 contributes between -166.1 and +95.9 points(!) to the win margin</li>
    </ul>
  </li>
  <li>`Distance Speed Zone 5 (m)` | +0.191, p-value = 0.627
    <ul>
      <li>Every additional metre a left wing performs in Speed Zone 5 contributes between -0.644 and 1.024 points to the win margin</li>
    </ul>
  </li>
</ol>

### Position 12: Inside centre

```{r backward stepwise selection 12, cache=TRUE}
pos12data <- fullyCombined[which(fullyCombined$Position == 12), -c(1:4)]
pos12data$`Work Recovery Ratio` <- droplevels(pos12data$`Work Recovery Ratio`)

pos12data <- pos12data[, -c(38, 40)]

corr12 <- cor(pos12data[, -c(11, 38)])
print("Which variables have high correlations with other variables?")
findCorrelation(corr12, cutoff = 0.999)
findCorrelation(corr12, cutoff = 0.9)
findCorrelation(corr12, cutoff = 0.8)
findCorrelation(corr12, cutoff = 0.75)
findCorrelation(corr12, cutoff = 0.7)
findCorrelation(corr12, cutoff = 0.68)

# Removing variables that are causing singularities
pos12data <- pos12data[, -c(1, 3:4, 6, 8:11, 13:15, 17, 19:20, 22:23, 26, 32, 34:35)]

model1.12 <- regsubsets(margins ~ ., data = pos12data, method = "backward", nvmax = 100)
coef(model1.12, 1:5)
full1.12 <- lm(margins ~ ., data = pos12data)
summary(full1.12)
confint(full1.12)[c(7, 5, 9, 17, 16), ]
```

These five models suggest that the most important GPS variables for an inside centre are, beginning from the most important:
<ol>
  <li>`Distance Speed Zone 1 (m)` | -0.00641, p-value = 0.0608
    <ul>
      <li>Every additional metre an inside centre covers in Speed Zone 1 contributes between -0.01314 and +0.00033 points to the win margin</li>
    </ul>
  </li>
  <li>`Athlete Load` | +1.37, p-value = 0.9016
    <ul>
      <li>Every additional point in Athlete Load an inside centre has contributes between -21.75 and +24.49 points to the win margin</li>
    </ul>
  </li>
  <li>`Sprints Speed Zone 4 (num)` | +16.8, p-value = 0.1443
    <ul>
      <li>Every additional sprint an inside centre performs in Speed Zone 4 contributes between -6.4 and +40.0 points(!) to the win margin</li>
    </ul>
  </li>
  <li>`Body Impacts Grade 2 (num)` | -14.2, p-value = 0.1003
    <ul>
      <li>Every additional Grade 2 body impact an inside centre performs contributes between -31.4 and +3.1 points(!) to the win margin</li>
    </ul>
  </li>
  <li>`Decelerations Zone 5 (num)` | +27.1, p-value = 0.6143
    <ul>
      <li>Every additional deceleration an inside centre performs in Deceleration Zone 5 contributes between -84.8 and +139.1 points(!) to the win margin</li>
    </ul>
  </li>
</ol>

### Position 13: Outside centre

```{r backward stepwise selection 13, cache=TRUE}
pos13data <- fullyCombined[which(fullyCombined$Position == 13), -c(1:4)]
pos13data$`Work Recovery Ratio` <- droplevels(pos13data$`Work Recovery Ratio`)
# All zeroes in Body Impacts Grade 3 (num)
pos13data <- pos13data[, -c(37:38, 40)]

corr13 <- cor(pos13data[, -c(11, 37)])
print("Which variables have high correlations with other variables?")
findCorrelation(corr13, cutoff = 0.999)
findCorrelation(corr13, cutoff = 0.9)
findCorrelation(corr13, cutoff = 0.8)
findCorrelation(corr13, cutoff = 0.75)
findCorrelation(corr13, cutoff = 0.7)
findCorrelation(corr13, cutoff = 0.67)

# Removing variables that are causing singularities
pos13data <- pos13data[, -c(1, 3:4, 6:8, 11:17, 19:21, 24, 26:27)]

model1.13 <- regsubsets(margins ~ ., data = pos13data, method = "backward", nvmax = 100)
coef(model1.13, 1:5)
full1.13 <- lm(margins ~ ., data = pos13data)
summary(full1.13)
confint(full1.13)[c(8, 16, 13, 11, 17), ]
```

These five models suggest that the most important GPS variables for an outside centre are, beginning from the most important:
<ol>
  <li>`Sprints Speed Zone 3 (num)` | +3.30, p-value = 0.157
    <ul>
      <li>Every additional sprint an outside centre performs in Speed Zone 3 contributes between -1.38 and +7.99 points to the win margin</li>
    </ul>
  </li>
  <li>`Body Impacts (num)` | -1.24, p-value = 0.266
    <ul>
      <li>Every additional body impact an outside centre performs in a match contributes between -3.52 and +1.03 points to the win margin</li>
    </ul>
  </li>
  <li>`Decelerations Zone 3 (num)` | -80.7, p-value = 0.350
    <ul>
      <li>Every additional deceleration an outside centre performs in Deceleration Zone 3 contributes between -257.2 and +95.7 points(!) to the win margin</li>
    </ul>
  </li>
  <li>`Accelerations Zone 4 (num)` | +53.5, p-value = 0.136
    <ul>
      <li>Every additional acceleration an outside centre performs in Acceleration Zone 4 contributes between -18.5 and +125.4 points(!) to the win margin</li>
    </ul>
  </li>
  <li>`Body Impacts Grade 1 (num)` | +22.6, p-value = 0.277
    <ul>
      <li>Every additional Grade 1 body impact an outside centre performs contributes between -19.7 and +64.8 points(!) to the win margin</li>
    </ul>
  </li>
</ol>

### Position 14: Right wing

```{r backward stepwise selection 14, cache=TRUE}
pos14data <- fullyCombined[which(fullyCombined$Position == 14), -c(1:4)]
pos14data$`Work Recovery Ratio` <- droplevels(pos14data$`Work Recovery Ratio`)

pos14data <- pos14data[, -c(38, 40)]

corr14 <- cor(pos14data[, -c(11, 38)])
print("Which variables have high correlations with other variables?")
findCorrelation(corr14, cutoff = 0.999)
findCorrelation(corr14, cutoff = 0.9)
findCorrelation(corr14, cutoff = 0.87)

# Removing variables that are causing singularities
pos14data <- pos14data[, -c(1, 4:6, 8, 10:11, 14:15, 26)]

model1.14 <- regsubsets(margins ~ ., data = pos14data, method = "backward", nvmax = 100)
coef(model1.14, 1:5)
full1.14 <- lm(margins ~ ., data = pos14data)
summary(full1.14)
confint(full1.14)[c(9, 28, 18, 27, 20), ]
```

These five models suggest that the most important GPS variables for a right wing are, beginning from the most important:
<ol>
  <li>`HIE Rate` | +102, p-value = 0.293
    <ul>
      <li>Every additional high intensity effort a right wing completes per unit of time contributes between -102 and +306 points(!) to the win margin</li>
    </ul>
  </li>
  <li>`Body Impacts Grade 3 (num)` | +282, p-value = 0.167
    <ul>
      <li>Every additional Grade 3 body impact a right wing performs contributes between -137 and +702 points(!) to the win margin</li>
    </ul>
  </li>
  <li>`Duration HR Zone 5 (s)` | +0.0761, p-value = 0.171
    <ul>
      <li>Every additional second a right wing remains in HR Zone 5 contributes between -0.0383 and +0.1905 points to the win margin</li>
    </ul>
  </li>
  <li>`Body Impacts Grade 2 (num)` | +67.7, p-value = 0.391
    <ul>
      <li>Every additional Grade 2 body impact a right wing performs contributes between -99.2 and +234.5 points to the win margin</li>
    </ul>
  </li>
  <li>`Accelerations Zone 4 (num)` | -29.3, p-value = 0.389
    <ul>
      <li>Every additional acceleration a right wing performs in Acceleration Zone 4 contributes between -101.0 and +42.5 points to the win margin</li>
    </ul>
  </li>
</ol>

### Position 15: Fullback

```{r backward stepwise selection 15, cache=TRUE}
pos15data <- fullyCombined[which(fullyCombined$Position == 15), -c(1:4)]
pos15data$`Work Recovery Ratio` <- droplevels(pos15data$`Work Recovery Ratio`)

pos15data <- pos15data[, -c(38, 40)]

corr15 <- cor(pos15data[, -c(11, 38)])
print("Which variables have high correlations with other variables?")
findCorrelation(corr15, cutoff = 0.999)
findCorrelation(corr15, cutoff = 0.9)
findCorrelation(corr15, cutoff = 0.8)
findCorrelation(corr15, cutoff = 0.7)
findCorrelation(corr15, cutoff = 0.65)
findCorrelation(corr15, cutoff = 0.64)

# Removing variables that are causing singularities
pos15data <- pos15data[, -c(1, 3:6, 8:10, 13:15, 17:27, 31, 33:35)]

model1.15 <- regsubsets(margins ~ ., data = pos15data, method = "backward", nvmax = 100)
coef(model1.15, 1:5)
full1.15 <- lm(margins ~ ., data = pos15data)
summary(full1.15)
confint(full1.15)[c(13, 5, 9, 8, 2), ]
```

These five models suggest that the most important GPS variables for a fullback are, beginning from the most important:
<ol>
  <li>`Body Impacts Grade 2 (num)` | +17.6, p-value = 0.315
    <ul>
      <li>Every additional Grade 2 body impact a fullback performs contributes between -18.0 and +53.1 points(!) to the win margin</li>
    </ul>
  </li>
  <li>`Work Recovery Ratio` | 1:2 -> +95.8, p-value = 0.645
    <ul>
      <li>If a fullback has a Work Recovery Ratio of 1:2 in a match, it will contribute between -331.8 and +523.3 points(!) to the win margin</li>
    </ul>
  </li>
  <li>`Accelerations Zone 3 (num)` | +30.1, p-value = 0.766
    <ul>
      <li>Every additional acceleration a fullback performs in Acceleration Zone 3 contributes between -177.5 and +237.7 points(!) to the win margin</li>
    </ul>
  </li>
  <li>`Hi Intensity Effort (num)` | -0.252, p-value = 0.778
    <ul>
      <li>For every additional effort a fullback performs that falls under any of the five high intensity categories (Hi-Int Sprints, Hi-Int Accelerations, Hi-Int Decelerations, Body Impacts and Jumps), between -2.094 and +1.590 points are added to the win margin</li>
    </ul>
  </li>
  <li>`Duration Speed Hi-Inten (s)` | +25.5, p-value = 0.777
    <ul>
      <li>Every additional second a fullback spends above the high intensity speed benchmark contributes between -159.8 and +210.9 points(!) to the win margin</li>
    </ul>
  </li>
</ol>

# Boosted trees using XGBoost


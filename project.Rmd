---
title: "Auckland Rugby Project - Trend Analysis of GPS Data"
author: "Ian Chen"
date: "30/07/2021"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r tidyverse, cache=TRUE, message=FALSE, warning=FALSE}
library(tidyverse)
```

First, I load the datasets in. The 2018 data notably has many more variables than the other years' data. These additional variables may be of use for the analysis, but because there is less data on these variables, they may necessitate a separate model that trains on only the 2018 data.

```{r loading the datasets, cache=TRUE}
# Loading the 2018 .csv files in
master2018 <- 
  list.files(path = "./2018_csvs/", pattern = "*.CSV", full.names = T) %>% 
  map_df(~read.csv(., skip = 4, header = TRUE))

master2019 <- 
  list.files(path = "./2019_csvs/", pattern = "*.CSV", full.names = T) %>% 
  map_df(~read.csv(., skip = 4, header = TRUE, colClasses = rep("character", 17)))

master2020 <- 
  list.files(path = "./2020_csvs/", pattern = "*.CSV", full.names = T) %>% 
  map_df(~read.csv(., skip = 4, header = TRUE))
# Getting the variable classes
varclasses <- unlist(lapply(master2018, class))
```

I begin with cleaning the 2018 .csv files.

The column names have varying degrees of spacing before and after words. They are cleaned up to have consistent names in proper English.

```{r cleaning up column names, cache=TRUE}
# Cleaning up column names
correctColumnNames <- c("Athlete", 
                        "Team", 
                        "Date", 
                        "Start Time", 
                        "Duration Total (s)", 
                        "Duration Speed Hi-Inten (s)", 
                        "Duration HR Hi-Inten (s)", 
                        "Distance Total (m)", 
                        "Distance Rate (m/min)", 
                        "Distance Speed Hi-Inten (m)", 
                        "Distance HR Hi-Inten (m)", 
                        "Speed Max (km/h)", 
                        "Sprints Total (num)", 
                        "Sprints Hi-Inten (num)", 
                        "Sprints HR Hi-Inten (num)", 
                        "HR Max Total (bpm)", 
                        "% Max HR", 
                        "Work Recovery Ratio", 
                        "Speed Duration Total (s)", 
                        "HR Duration Total (s)", 
                        "Athlete Load", 
                        "Metabolic PowerPeak", 
                        "Hi Int Acceleration (num)", 
                        "Hi Int Deceleration (num)", 
                        "Impact Rate (imp/min)", 
                        "Body Impacts (num)", 
                        "Hi Intensity Effort (num)", 
                        "HIE Rate", 
                        "Distance Speed Zone 1 (m)", 
                        "Distance Speed Zone 2 (m)", 
                        "Distance Speed Zone 3 (m)", 
                        "Distance Speed Zone 4 (m)", 
                        "Distance Speed Zone 5 (m)", 
                        "Sprints Speed Zone 3 (num)", 
                        "Sprints Speed Zone 4 (num)", 
                        "Sprints Speed Zone 5 (num)", 
                        "Duration HR Zone 4 (s)", 
                        "Duration HR Zone 5 (s)", 
                        "Accelerations Zone 3 (num)", 
                        "Accelerations Zone 4 (num)", 
                        "Accelerations Zone 5 (num)", 
                        "Decelerations Zone 3 (num)", 
                        "Decelerations Zone 4 (num)", 
                        "Decelerations Zone 5 (num)", 
                        "Body Impacts in Body Impacts Zone Total (num)",
                        "Body Impacts Grade 1 (num)", 
                        "Body Impacts Grade 2 (num)", 
                        "Body Impacts Grade 3 (num)", 
                        "Body Impacts Grade 4 (num)", 
                        "Body Impacts Grade 5 (num)")
colnames(master2018) <- correctColumnNames
```

Each individual .csv includes four opening rows that do not provide any meaningful information (which are skipped when the .csv is read into R), and three rows at the end that provide details about the average (mean), maximum and minimum values for each column. These are not useful for this analysis, so they should be removed here.

Furthermore, there are many data points that have missing data (represented by two asterisks - "**"). These need to be converted into NA values, which are easier to work around than a string of two asterisks forcing numeric columns into character columns.

```{r removing excess rows and converting missing values, cache=TRUE}
# Removing excess rows
master2018 <- subset(master2018, Athlete != "Avg" & Athlete != "Highest" & Athlete != "Lowest")

# Replacing all missing values with NA
master2018 <- na_if(master2018, "**")
```

The cells that were initially occupied by "**" strings forcibly converted their respective columns into character columns during the dataset import. These columns need to be converted into their proper class such that they can be useful for modeling.

First, the dates are imported into R as characters. For ease of reading, the data frame is sorted by date, from earliest to latest. This involves the conversion of the `Date` column into Date class objects, which requires all values in the `Date` column to be of a certain format.

The last column appears to be an error, not existing in the actual .csv files, so it is additionally dropped.

```{r conversion into proper classes, cache=TRUE}
# Converting dates into something usable
library(lubridate)
master2018$Date[21:38] <- "27/10/2018"
master2018$Date <- parse_date_time(master2018$Date, c("%d/%m/%Y"))
# Sorting by date, dropping redundant column `X`
master2018 <- master2018[order(as.Date(master2018$Date)), -51]
```

Columns 11 and 15 (`Distance HR Hi-Inten (m)` and `Sprints HR Hi-Inten (num)` respectively) are numeric values that were also imported into R as characters. These are transformed back into numeric variables. This is necessary for the proportional standardisation that is applied later.

```{r conversion into proper classes 2, cache=TRUE}
# Converting columns 11 and 15 back into numeric vectors
for (i in c(11, 15)) {
  master2018[, i] <- as.numeric(master2018[, i])
}
```

All durations are imported into R as character strings, as R can't parse the "MM:SS" format. Some preprocessing will need to be done with the times in the dataset, and by converting them into numeric values, manipulation of them will become a lot simpler. Therefore, all times in the data are converted to numeric values.

In this case, converting them to seconds is an easy way of standardising all of the times, making them integers. Integers make things easy to calculate without having to deal with fractions of a minute (which are in base 60).

```{r converting the times to seconds, cache=TRUE}
minsec_to_sec <- function(strvec) {
  # All durations are in "MM:SS" format; durations > 1 hr simply have MM > 59
  prelength <- ifelse(nchar(strvec) == 6, 3, ifelse(nchar(strvec) == 5, 2, 1))
  pre <- as.numeric(substr(strvec, 1, prelength))
  suf <- as.numeric(substr(strvec, nchar(strvec) - 1, nchar(strvec)))
  strvec <- pre * 60 + suf
  return(strvec)
}
master2018[, c(5:7, 19:20, 37:38)] <- lapply(master2018[, c(5:7, 19:20, 37:38)], minsec_to_sec)
```

A rugby union match goes for two 40-minute halves, with a halftime of a maximum length of 15 minutes. This sets a match at roughly a maximum of 95 minutes long. In the Mitre 10 Cup, should a semi-final or final match be tied at the end of regulation time, two 10-minute halves of extra time are played. This is the longest extension a Mitre 10 Cup game can have. Because much of the data's time values are abnormally high, a hard limit is set at 95 minutes (roughly the length of a regular match, including halftime), with the exception of the 2018 final, which went to extra time (resulting in a total of 120 minutes being played, so a hard limit of 120 minutes will be applied exclusively for that match).

95 minutes is equal to $95 \times 60 = 5700$ seconds, while 120 minutes is equal to $120 \times 60 = 7200$ seconds, so 5700 and 7200 will be the hard limits imposed on the minutes played.

Other duration variables may also have abnormally high values, so they will need to be adjusted too. These anomalous values are likely due to errors with the time tracking device, as it appears that many of the duration values are problematic.

If a player's total minutes played is cut down to the set ceiling, then the other duration variables are adjusted by calculating a proportion of the original minutes played, and using this proportion as a multiplier for the other duration variables. For instance, if a player has 100 minutes (6000 seconds) played in a non-2018-final match, that player's corresponding proportion is $5700/6000 = 0.95$, which then multiplies by the player's other duration values to give their adjusted values.

```{r adjusting the duration values, cache=TRUE}
# Calculating proportion by the above method
master2018$Proportion <- ifelse(
  as.character(master2018[, 3]) == "2018-10-27", 
  7200 / master2018$`Duration Total (s)`, 
  5700 / master2018$`Duration Total (s)`)
# Only interested in adjusting values that have a `Proportion` value < 1
master2018$Proportion[which(master2018$Proportion > 1)] <- 1
for (j in c(5, 7:8, 10:11, 13:15, 19:24, 26:27, 29:50)) {
  master2018[, j] <- master2018[, j] * master2018$Proportion
}
```

Column 17, `% Max HR`, contains a percentage symbol in each of the values. Because all of these values should be numeric, the percentage symbol is removed and `% Max HR` is converted to numeric.

```{r removing percentage symbols, cache=TRUE}
# Removing percentage symbols
master2018[, 17] <- as.numeric(substr(master2018[, 17], 1, nchar(master2018[, 17]) - 1))
```

Column 18, `Work Recovery Ratio`, contains a small set of unique values. This can be recoded into a factor.

```{r recoding work recovery ratio, cache=TRUE}
# Recoding Work Recovery Ratio into a factor
master2018[, 18] <- as.factor(master2018[, 18])
```

Player names are misspelled in different ways across each dataset. These must be standardised to allow for simpler merging of additional information.

```{r standardising names, cache=TRUE}
# Every name from every dataset combined
currentNames <- sort(unique(c(master2018$Athlete, master2019$Athlete, master2020$Athlete)))

# The incorrectly-recorded names
problematicNames <- c("Able, Rob", 
                      "Hallem Ewes, Liam", 
                      "Hodgmen, Alex", 
                      "Lemalu, Faatungu", 
                      "Liaana, Desma", 
                      "Liana, Desma", 
                      "Lundenmuth, Ezeikeil", 
                      "Reidler Kapa, Waimana", 
                      "Ruru, Jonathon", 
                      "Schwenke, Lief", 
                      "Scraffton, Scott", 
                      "Sosene, Mike", 
                      "Sotutu, Hoksins")
# The corrections to the above names
correctedNames <- c("Abel, Robbie", 
                    "Hallam-Eames, Liam", 
                    "Hodgman, Alex", 
                    "Lemalu, Fa'atiga", 
                    "Liaina, Desma", 
                    "Liaina, Desma", 
                    "Lindenmuth, Ezi", 
                    "Riedlinger-Kapa, Waimana", 
                    "Ruru, Jonathan", 
                    "Schwenke, Leif", 
                    "Scrafton, Scott", 
                    "Sosene-Feagai, Mike", 
                    "Sotutu, Hoskins")

# A function for name correction
nameCorrection <- function(data) {
  for (k in 1:length(problematicNames)) {
    data[which(data[, 1] == problematicNames[k]), 1] <- correctedNames[k]
  }
  return(data)
}

# Applying the function
master2018 <- nameCorrection(master2018)
```

Win margins will be used as a one-size-fits-all metric for measuring how good a player's performance in a match is i.e. the response variable for any fitted model. This is added to the main dataset.

```{r match win margin data, cache=TRUE}
# Dates of matches
matchDates <- as.Date(c("2018-08-18", 
                        "2018-08-26", 
                        "2018-08-30", 
                        "2018-09-07", 
                        "2018-09-16", 
                        "2018-09-22", 
                        "2018-09-28", 
                        "2018-10-04", 
                        "2018-10-10", 
                        "2018-10-14", 
                        "2018-10-20", 
                        "2018-10-27", 
                        "2019-08-09", 
                        "2019-08-15", 
                        "2019-08-24", 
                        "2019-08-31", 
                        "2019-09-08", 
                        "2019-09-14", 
                        "2019-09-22", 
                        "2019-09-27", 
                        "2019-10-05", 
                        "2019-10-11", 
                        "2019-10-19", 
                        "2020-09-12", 
                        "2020-09-20", 
                        "2020-09-27", 
                        "2020-10-02", 
                        "2020-10-10", 
                        "2020-10-17", 
                        "2020-10-24", 
                        "2020-10-31", 
                        "2020-11-07", 
                        "2020-11-15", 
                        "2020-11-21", 
                        "2020-11-28"))
# Match win margins by date
margins <- c(4, 16, 18, 26, 5, 1, -5, 5, 48, 16, 21, 7, 
             0, 33, 6, 0, -10, 15, -19, -40, 57, 24, -9, 
             32, -18, 38, 4, 1, 21, -1, 21, 4, -1, 5, -1)
# Combining date and win margins into one dataframe
winMargins <- data.frame(Date = matchDates, margins)
# Combining win margins into the main dataframe, merging by Date
master2018 <- left_join(master2018, winMargins)
```

I created two supplementary files to provide additional necessary variables. The first is `positional_data_by_match.csv`, which contains each match's game day squad. This provides the position that each player named in the squad for that matchup played at. The replacements (wearing jerseys 16-23) were labelled as 16, as the replacement jersey number does not provide exact positional information.

The second supplementary file is `positional data.csv`, which contains the preferred position for each player. This was determined by selecting the position in the starting XV that they appeared in the most over the matches represented in the dataset. For those that did not make any appearances in the starting XV, some Googling and some clarification with Paul Downes, my Auckland Rugby representative filled in their preferred position.

The positional data in `positional_data_by_match.csv` is added to the master dataset for the players that were named in the starting XV for each of the matches played in 2018. The preferred positions in `positional data.csv` is added to the master dataset for any players that were named as replacements, to show what position they would typically fill in if they had started the match with the starting XV.

```{r positional data by match, cache=TRUE}
# Positional data by match
matchPos <- read.csv("positional_data_by_match.csv", skip = 4)
# Rename columns to be consistent with the data
colnames(matchPos) <- c("Athlete", as.character(matchDates))
matchPos <- nameCorrection(matchPos)
# Initialise the position column
master2018$Position <- 0
# Go through each match day
for (l in 2:36) {
  currentDate <- colnames(matchPos)[l]
  # Get the squad that played on/was named for this day
  activeSquad <- matchPos[which(matchPos[, l] != 0), c(1, l)]
  playersOnThisDate <- which(master2018$Date == as.Date(currentDate) & master2018$Athlete %in% activeSquad[, 1])
  # Add the position for each player
  for (m in playersOnThisDate) {
    player <- which(activeSquad[, 1] == master2018[m, 1])
    master2018[m, 53] <- activeSquad[player, 2]
  }
}

# Now to deal with the replacements, which are all labelled 16
# The preferred positions are in "positional data.csv"
preferredPos <- read.csv("positional data.csv")[1:65, ]
colnames(preferredPos) <- c("Name", 
                            "1 - Loosehead prop", 
                            "2 - Hooker", 
                            "3 - Tighthead prop", 
                            "4 - Left lock", 
                            "5 - Right lock", 
                            "6 - Blindside flanker", 
                            "7 - Openside flanker", 
                            "8 - Number 8", 
                            "9 - Scrum-half", 
                            "10 - Fly-half", 
                            "11 - Left wing", 
                            "12 - Inside centre", 
                            "13 - Outside centre", 
                            "14 - Right wing", 
                            "15 - Fullback", 
                            "16-23 - Replacement", 
                            "Pref. pos. (number)", 
                            "Pref. pos. (text)", 
                            "Pref. group")
preferredPos <- nameCorrection(preferredPos)
# Coercing the columns of the preferred positional data into ideal classes
for (n in 2:18) {
  preferredPos[, n] <- as.numeric(preferredPos[, n])
}
# Getting the rows that correspond to replacements
replacements <- which(master2018$Position == 16)
# Giving the replacements their preferred position
for (o in replacements) {
  replacementName <- master2018[o, 1]
  master2018[o, 53] <- as.numeric(preferredPos[which(preferredPos[, 1] == replacementName), 18])
}
# Converting the positional data into a factor
master2018[, 53] <- as.factor(master2018[, 53])
```

Finally, excess rows are removed from the master dataset. These include rows where, for a given match, a player is listed multiple times, as well as players who are in the data but were not named to the 23-man match-day squad for that match. For the latter, they did not contribute to the win margin that corresponds to that match, so their data is not useful for prediction.

```{r removing excess rows, cache=TRUE}
# Removing duplicate rows
for (p in unique(master2018$Date)) {
  matchPlayers <- master2018[which(master2018$Date == p), 1]
  for (q in matchPlayers[duplicated(matchPlayers)]) {
    dupes <- master2018[which(master2018$Date == p & master2018$Athlete == q),]
    notHighestMinutes <- as.numeric(rownames(dupes[which(dupes$`Duration Total (s)` != max(dupes$`Duration Total (s)`)), ]))
    master2018 <- master2018[-notHighestMinutes,]
  }
}
# Deleting players that didn't play in the game, since there would be no win margin associated with their stats
master2018 <- master2018[-which(master2018$Position == 0),]
```

That should be all the preliminary cleaning and preprocessing that needs to be done. The same methods are applied to the 2019 data, although in a modified manner. The 2019 data has a far smaller subset of the variables in the 2018 data, so the column names here are different than the 2018 columns. Otherwise, the same cleaning is applied to the 2019 data, to keep the data consistent across each year.

```{r 2019, cache=TRUE}
# 2019 and 2020 datasets have fewer variables
colnames2019_20 <- c("Athlete", 
                    "Team", 
                    "Date", 
                    "Start Time", 
                    "Duration Total (s)", 
                    "Distance Total (m)", 
                    "Speed Max (km/h)", 
                    "Hi Int Acceleration (num)", 
                    "Distance Speed Zone 1 (m)", 
                    "Distance Speed Zone 2 (m)", 
                    "Distance Speed Zone 3 (m)", 
                    "Distance Speed Zone 4 (m)", 
                    "Distance Speed Zone 5 (m)", 
                    "Body Impacts in Body Impacts Zone Total (num)", 
                    "Sprints Speed Zone 3 (num)", 
                    "Sprints Speed Zone 4 (num)", 
                    "Sprints Speed Zone 5 (num)")
colnames(master2019) <- colnames2019_20

master2019 <- subset(master2019, Athlete != "Avg" & Athlete != "Highest" & Athlete != "Lowest")
master2019 <- na_if(master2019, "**")

master2019$Date <- parse_date_time(master2019$Date, c("%d/%m/%Y"))
master2019 <- master2019[order(as.Date(master2019$Date)), -18]

for (i in 6:17) {
  master2019[, i] <- as.numeric(master2019[, i])
}

master2019[, 5] <- minsec_to_sec(master2019[, 5])

master2019$Proportion <- 5700 / master2019$`Duration Total (s)`
master2019$Proportion[which(master2019$Proportion > 1)] <- 1
for (j in c(5:6, 8:17)) {
  master2019[, j] <- master2019[, j] * master2019$Proportion
}

master2019 <- nameCorrection(master2019) %>% 
  left_join(winMargins)

master2019$Position <- 0
for (l in 2:36) {
  currentDate <- colnames(matchPos)[l]
  activeSquad <- matchPos[which(matchPos[, l] != 0), c(1, l)]
  playersOnThisDate <- which(master2019$Date == as.Date(currentDate) & master2019$Athlete %in% activeSquad[, 1])
  for (m in playersOnThisDate) {
    player <- which(activeSquad[, 1] == master2019[m, 1])
    master2019[m, 20] <- activeSquad[player, 2]
  }
}
replacements <- which(master2019$Position == 16)
for (o in replacements) {
  replacementName <- master2019[o, 1]
  master2019[o, 20] <- as.numeric(preferredPos[which(preferredPos[, 1] == replacementName), 18])
}
master2019[, 20] <- as.factor(master2019[, 20])

for (p in unique(master2019$Date)) {
  matchPlayers <- master2019[which(master2019$Date == p), 1]
  for (q in matchPlayers[duplicated(matchPlayers)]) {
    dupes <- master2019[which(master2019$Date == p & master2019$Athlete == q),]
    notHighestMinutes <- as.numeric(rownames(dupes[which(dupes$`Duration Total (s)` != max(dupes$`Duration Total (s)`)), ]))
    master2019 <- master2019[-notHighestMinutes,]
  }
}
master2019 <- master2019[-which(master2019$Position == 0),]
```

The same is done with the 2020 dataset, which closely resembles the 2019 dataset in structure and format, including the number and name of columns.

```{r 2020, cache=TRUE}
colnames(master2020) <- colnames2019_20

master2020 <- subset(master2020, Athlete != "Avg" & Athlete != "Highest" & Athlete != "Lowest")
master2020 <- na_if(master2020, "**")

master2020$Date <- parse_date_time(master2020$Date, c("%d/%m/%Y"))
master2020 <- master2020[order(as.Date(master2020$Date)), -18]

for (i in 6:17) {
  master2020[, i] <- as.numeric(master2020[, i])
}

master2020[, 5] <- minsec_to_sec(master2020[, 5])

master2020$Proportion <- 5700 / master2020$`Duration Total (s)`
master2020$Proportion[which(master2020$Proportion > 1)] <- 1
for (j in c(5:6, 8:17)) {
  master2020[, j] <- master2020[, j] * master2020$Proportion
}

master2020 <- nameCorrection(master2020) %>% 
  left_join(winMargins)

master2020$Position <- 0
for (l in 2:36) {
  currentDate <- colnames(matchPos)[l]
  activeSquad <- matchPos[which(matchPos[, l] != 0), c(1, l)]
  playersOnThisDate <- which(master2020$Date == as.Date(currentDate) & master2020$Athlete %in% activeSquad[, 1])
  for (m in playersOnThisDate) {
    player <- which(activeSquad[, 1] == master2020[m, 1])
    master2020[m, 20] <- activeSquad[player, 2]
  }
}
replacements <- which(master2020$Position == 16)
for (o in replacements) {
  replacementName <- master2020[o, 1]
  master2020[o, 20] <- as.numeric(preferredPos[which(preferredPos[, 1] == replacementName), 18])
}
master2020[, 20] <- as.factor(master2020[, 20])

for (p in unique(master2020$Date)) {
  matchPlayers <- master2020[which(master2020$Date == p), 1]
  for (q in matchPlayers[duplicated(matchPlayers)]) {
    dupes <- master2020[which(master2020$Date == p & master2020$Athlete == q),]
    notHighestMinutes <- as.numeric(rownames(dupes[which(dupes$`Duration Total (s)` != max(dupes$`Duration Total (s)`)), ]))
    master2020 <- master2020[-notHighestMinutes,]
  }
}
master2020 <- master2020[-which(master2020$Position == 0),]
```

The 2019 and 2020 datasets are combined, since they share the same columns. The 2018 dataset is joined with them also, but only at the columns that are shared with the 2019 and 2020 datasets.

```{r combining the datasets, cache=TRUE}
combinedData <- rbind(master2018[, c(1:5, 8, 12, 23, 29:36, 45, 51:53)], master2019, master2020)
```

Finally, it appears that `Speed Duration Total (s)` and `HR Duration Total (s)` are not needed, since they measure the total duration of data collected beginning when the GPS unit locks (and when heart rate is detected for `HR Duration Total (s)`.) These are extremely correlated with `Duration Total (s)`, so these can be safely removed.

Additionally, `Body Impacts in Body Impact Zones Total (num)` is equal to the `Body Impacts (num)` measure in the 2018 dataset, appearing to capture the same information. Because the former is in all three datasets, the latter is removed, and the former is renamed to the simpler `Body Impacts (num)`.

```{r removing some duration variables, cache=TRUE}
master2018 <- master2018[, -c(19, 20, 26)]
colnames(master2018)[44] <- "Body Impacts (num)"
```

# Exploring the data

I want to explore the data to see if there are any interesting relationships between positional groups for each of the variables present in the datasets.

First, the variables unique to the 2018 dataset are plotted.

```{r exploring data 2018, cache=TRUE}
# 2018 dataset-unique variable visualisation
for (u in c(6:7, 9:11, 13:17, 19:20, 22:25, 34:41, 43:47)) {
  print(ggplot(master2018, aes(Position, master2018[, u])) + 
          geom_boxplot() + 
          geom_point(alpha = 0.3) + 
          ylab(colnames(master2018)[u]) + 
          geom_jitter())
}
```

There are a lot of plots here, but the main points I gleaned from these were that:

<ul>
  <li>Backs have high values for most sprint, distance and deceleration stats;</li>
  <li>Forwards tend to have high values for low grade body impacts, and low values for energy related measures;</li>
  <li>Heart rate threshold-related measures (e.g. `Duration HR Hi-Inten (s)`, `Distance HR Hi-Inten (m)`, `Sprints HR Hi-Inten (num)`) do not have any particular trend outside of inside centre (#12) and fullback (#15) registering high values;</li>
  <li>There are some observations for the heart rate measures `HR Max Total (bpm)` and `% Max HR` that are zeroes, which are unrealistic (as this would imply the players died - more likely to be due to a malfunction in the GPS unit);</li>
  <li>Scrum-half (#9) has a much higher high-intensity sprint number than every other position;</li>
  <li>Inside centre (#12) registers particularly high values for accelerations, decelerations and body impacts, but also high values overall for most of the other variables.</li>
</ul>

Now, the remaining variables shared between the 2018, 2019 and 2020 datasets are plotted.

```{r exploring data all datasets, cache=TRUE}
# Plotting the other variables
for (v in 5:17) {
  print(ggplot(combinedData, aes(Position, combinedData[, v])) + 
          geom_boxplot() + 
          geom_point(alpha = 0.2) + 
          ylab(colnames(combinedData)[v]) + 
          geom_jitter())
}
```

These plots mostly reinforce what is in the 2018 dataset-unique variable plots, but additionally:

<ul>
  <li>Front rowers (#1, #2, #3) have lower minutes and distance values than the other positions;</li>
  <li>For `Body Impacts in Body Impacts Zone Total (num)`, inside centre (#12) has a high distribution centre relative to the other backs. Overall, 12 has the second-highest distribution centre, which also contains the data points with the highest values across all positions.</li>
</ul>

# Finding the positional maximum, minimum and mean data for each marker

The 2018 dataset contains 53 variables, 33 of which are not shared by the 2019 and 2020 datasets. Within the 20 variables that are shared, 7 of them are not performance markers; they are either redundant information, unique identifiers for each individual row, or variables I created during the cleaning process.

Therefore, the 2018 dataset must be separated by position, and then the positional maximum, minimum and mean can be found. These values are printed. Variable 18, `HIE Rate` is not numeric, so it is left out here. Any performance markers that are shared by the 2018, 2019 and 2020 datasets are then found and printed afterwards.

```{r positional 2018, cache=TRUE}
for (r in 1:15) {
  # Finding the positional minimum, mean and maximum for performance markers exclusive to the 2018 dataset
  positionalData <- master2018[which(master2018$Position == r),]
  cat(paste0("POSITION: ", as.character(r)), "\n")
  for (s in c(6:7, 9:11, 13:17, 19:20, 22:25, 34:41, 43:47)) {
    print(paste0("Variable ", as.character(s), " - ", colnames(positionalData)[s], 
                 " - MIN: ", min(positionalData[, s], na.rm = TRUE), 
                 " | MEAN: ", mean(positionalData[, s], na.rm = TRUE), 
                 " | MAX: ", max(positionalData[, s], na.rm = TRUE)))
  }
  cat("\n")
  # Finding the positional minimum, mean and maximum for performance markers shared between the 2018, 2019 and 2020 datasets
  positionalSmallData <- combinedData[which(combinedData$Position == r),]
  for (t in 5:17) {
    print(paste0("Variable ", as.character(t), " - ", colnames(positionalSmallData)[t], 
                 " - MIN: ", min(positionalSmallData[, t], na.rm = TRUE), 
                 " | MEAN: ", mean(positionalSmallData[, t], na.rm = TRUE), 
                 " | MAX: ", max(positionalSmallData[, t], na.rm = TRUE)))
  }
  cat("\n", "\n", "\n")
}
```

# Fitting models to find top variables by position

To find the top variables by position, models need to be fitted. The easiest way to do this is to fit a separate model for the data filtered by each position.

```{r models, cache=TRUE}
library(caret)
fullyCombined <- full_join(master2018, combinedData)
dim(fullyCombined)
fullyCombined$Position <- droplevels(fullyCombined$Position)
levels(fullyCombined$`Work Recovery Ratio`) <- c(levels(fullyCombined$`Work Recovery Ratio`), "Not Applicable")
fullyCombined[which(is.na(fullyCombined$`Work Recovery Ratio`)), 18] <- "Not Applicable"
fullyCombined$`Work Recovery Ratio` <- relevel(fullyCombined$`Work Recovery Ratio`, "Not Applicable")
fullyCombined$`Work Recovery Ratio` <- droplevels(fullyCombined$`Work Recovery Ratio`)
head(fullyCombined)
fullyCombined <- fullyCombined[, -c(nearZeroVar(fullyCombined))]
dim(fullyCombined)

for (u in 1:15) {
  imputations <- preProcess(fullyCombined[which(fullyCombined$Position == u), ], method = "medianImpute")
  fullyCombined[which(fullyCombined$Position == u), ] <- predict(imputations, fullyCombined[which(fullyCombined$Position == u), ])
}

# imputations <- preProcess(fullyCombined, method = "medianImpute")
# data <- predict(imputations, fullyCombined)
```

## Backward stepwise selection

For backward stepwise selection, the datasets are split by position. A full model is fitted for each split to obtain the coefficients for the variables when all are taken into account. Some variables may result in singularities, which are most likely due to highly correlated variables coexisting in the dataset. By creating a correlation matrix with `cor()`, and finding variables with correlations beyond a certain cutoff using `findCorrelation()`, these variables can be singled out and removed from the data.

The top five variables by backward stepwise selection are then determined with `regsubsets(..., method = "backward")`. The most important variable is removed last, and as such is the only variable in the one-variable model. The second-most important variable is removed penultimately, and as such is the variable that differs between the one-variable and two-variable models, etc. The full model coefficients for these variables can then be determined and analysed.

### Position 1: Loosehead prop

```{r backwards stepwise selection 1, cache=TRUE}
library(leaps)

pos1data <- fullyCombined[which(fullyCombined$Position == 1), -c(1:4)]
pos1data$`Work Recovery Ratio` <- droplevels(pos1data$`Work Recovery Ratio`)
pos1data <- pos1data[, -c(40, 42)]

full1.1 <- lm(margins ~ . , data = pos1data)
summary(full1.1)

corr1 <- cor(pos1data[, -c(13, 40)])
print("Which variables have high correlations with other variables?")
findCorrelation(corr1, cutoff = 0.999)
findCorrelation(corr1, cutoff = 0.99)
findCorrelation(corr1, cutoff = 0.95)
findCorrelation(corr1, cutoff = 0.9)
findCorrelation(corr1, cutoff = 0.85)
findCorrelation(corr1, cutoff = 0.8)
findCorrelation(corr1, cutoff = 0.75)
findCorrelation(corr1, cutoff = 0.7)

# Removing variables that are causing singularities
pos1data <- pos1data[, -c(3, 4, 8, 10, 12:13, 15:17, 19, 22, 25:26, 37)]

model1.1 <- regsubsets(margins ~ ., data = pos1data, method = "backward", nvmax = 100)
coef(model1.1, c(1:5))
```

These five models suggest that the most important GPS variables for a loosehead prop are, beginning from the most important:
<ol>
  <li>Distance Speed Zone 5 (m)
    <ul>
      <li>Every additional metre a loosehead prop covers in Speed ZOne 5 contributes +32.9 points to the win margin</li>
    </ul>
  </li>
  <li>Distance Speed Zone 4 (m)
    <ul>
      <li>Every additional metre a loosehead prop covers in Speed Zone 4 contributes -1.65 points to the win margin</li>
    </ul>
  </li>
  <li>Duration Speed Hi-Inten (s)
    <ul>
      <li>Every additional second a loosehead prop manages to maintain a speed beyond the high intensity speed threshold contributes -49.3 points to the win margin</li>
    </ul>
  </li>
  <li>Sprints Speed Zone 5 (num)
    <ul>
      <li>Every additional sprint a loosehead prop performs in Speed Zone 5 contributes -77.3 points to the win margin</li>
    </ul>
  </li>
  <li>Duration HR Zone 4 (s)
    <ul>
      <li>Every additional second a loosehead prop spends in HR Zone 4 contributes +0.0655 points to the win margin</li>
    </ul>
  </li>
</ol>

### Position 2: Hooker

```{r backwards stepwise selection 2, cache=TRUE}
pos2data <- fullyCombined[which(fullyCombined$Position == 2), -c(1:4)]
pos2data$`Work Recovery Ratio` <- droplevels(pos2data$`Work Recovery Ratio`)
# All zeroes for Decelerations Zone 5 (num)
pos2data <- pos2data[, -c(35, 40, 42)]

corr2 <- cor(pos2data[, -c(13, 40)])
print("Which variables have high correlations with other variables?")
findCorrelation(corr2, cutoff = 0.999)
findCorrelation(corr2, cutoff = 0.99)
findCorrelation(corr2, cutoff = 0.98)

# Removing variables that are causing singularities
pos2data <- pos2data[, -c(6, 10, 12, 19)]

model1.2 <- regsubsets(margins ~ ., data = pos2data, method = "backward", nvmax = 100)
coef(model1.2, 1:5)
full1.2 <- lm(margins ~ ., data = pos2data)
summary(full1.2)
```

These five models suggest that the most important GPS variables for a hooker are, beginning from the most important:
<ol>
  <li>Distance Speed Zone 2 (m)
    <ul>
      <li>Every additional metre a hooker covers in Speed Zone 2 contributes +6.84 points to the win margin</li>
    </ul>
  </li>
  <li>Distance Total (m)
    <ul>
      <li>Every additional metre a hooker covers in a match contributes -3.12 points to the win margin</li>
      <li>Note that every other distance variable in the full model has a positive coefficient</li>
    </ul>
  </li>
  <li>Distance Speed Zone 1 (m)
    <ul>
      <li>Every additional metre a hooker covers in Speed Zone 1 contributes +6.86 points to the win margin</li>
    </ul>
  </li>
  <li>Distance Speed Zone 4 (m)
    <ul>
      <li>Every additional metre a hooker covers in Speed Zone 4 contributes +7.81 points to the win margin</li>
    </ul>
  </li>
  <li>Distance Speed Zone 3 (m)
    <ul>
      <li>Every additional metre a hooker covers in Speed Zone 3 contributes +6.97 points to the win margin</li>
    </ul>
  </li>
</ol>

### Position 3: Tighthead prop

```{r backwards stepwise selection 3, cache=TRUE}
pos3data <- fullyCombined[which(fullyCombined$Position == 3), -c(1:4)]
pos3data$`Work Recovery Ratio` <- droplevels(pos3data$`Work Recovery Ratio`)
# All zeroes for Sprints Speed Zone 5 (num), Decelerations Zones 4 and 5 (num)
pos3data <- pos3data[, -c(27, 34, 35, 40, 42)]

corr3 <- cor(pos3data[, -c(13, 40)])
print("Which variables have high correlations with other variables?")
findCorrelation(corr3, cutoff = 0.999)
findCorrelation(corr3, cutoff = 0.99)
findCorrelation(corr3, cutoff = 0.9)
findCorrelation(corr3, cutoff = 0.85)
findCorrelation(corr3, cutoff = 0.8)
findCorrelation(corr3, cutoff = 0.75)
findCorrelation(corr3, cutoff = 0.7)
findCorrelation(corr3, cutoff = 0.67)

# Removing variables that are causing singularities
pos3data <- pos3data[, -c(3, 4, 6, 8, 11, 13:20, 24:26, 27, 33)]

model1.3 <- regsubsets(margins ~ ., data = pos3data, method = "backward", nvmax = 100)
coef(model1.3, 1:5)
full1.3 <- lm(margins ~ ., data = pos3data)
summary(full1.3)
```

These five models suggest that the most important GPS variables for a tighthead prop are, beginning from the most important:
<ol>
  <li>Duration HR Zone 5 (s)
    <ul>
      <li>Every additional second a tighthead prop spends in HR Zone 5 contributes +0.459 points to the win margin</li>
    </ul>
  </li>
  <li>Speed Max (km/h)
    <ul>
      <li>For every kilometre per hour in a tighthead prop's maximum speed in a match, -1.15 points are added to the win margin</li>
    </ul>
  </li>
  <li>% Max HR
    <ul>
      <li>For every percentage point of a tighthead prop's match maximum heart rate relative to their maximum heart rate benchmark, -10.0 points are added to the win margin</li>
    </ul>
  </li>
  <li>Decelerations Zone 3 (num)
    <ul>
      <li>Every additional deceleration a tighthead prop performs in Deceleration Zone 3 contributes +258 points(!) to the win margin</li>
    </ul>
  </li>
  <li>Body Impacts Grade 1 (num)
    <ul>
      <li>Every additional Grade 1 body impact a tighthead prop performs contributes -121 points(!) to the win margin</li>
    </ul>
  </li>
</ol>

The tighthead prop data contains some surprisingly high coefficient magnitudes. This may be due to the number of variables removed from the dataset to deal with singularities in the full model.

### Position 4: Left lock

```{r backwards stepwise selection 4, cache=TRUE}
pos4data <- fullyCombined[which(fullyCombined$Position == 4), -c(1:4)]
pos4data$`Work Recovery Ratio` <- droplevels(pos4data$`Work Recovery Ratio`)
# All zeroes in Duration Speed Hi-Inten (s)
pos4data <- pos4data[, -c(2, 40, 42)]

corr4 <- cor(pos4data[, -c(12, 39)])
print("Which variables have high correlations with other variables?")
findCorrelation(corr4, cutoff = 0.999)
findCorrelation(corr4, cutoff = 0.9)
findCorrelation(corr4, cutoff = 0.8)
findCorrelation(corr4, cutoff = 0.75)
findCorrelation(corr4, cutoff = 0.7)

# Removing variables that are causing singularities
pos4data <- pos4data[, -c(1, 5, 6, 8, 9, 11, 12, 14:18, 21, 22, 25, 27, 28, 33)]

model1.4 <- regsubsets(margins ~ ., data = pos4data, method = "backward", nvmax = 100)
coef(model1.4, 1:5)
full1.4 <- lm(margins ~ ., data = pos4data)
summary(full1.4)
```

These five models suggest that the most important GPS variables for a left lock are, beginning from the most important:
<ol>
  <li>Distance Speed Zone 1 (m)
    <ul>
      <li>Every additional metre a left lock covers in Speed Zone 1 contributes +0.498 to the win margin</li>
    </ul>
  </li>
  <li>Distance Total (m)
    <ul>
      <li>Every additional metre a left lock covers in the match contributes -0.494 points to the win margin</li>
      <li>Note that all other distance variables except `Distance Rate (m/min)` in the full model has a positive coefficient</li>
    </ul>
  </li>
  <li>Distance Speed Zone 2 (m)
    <ul>
      <li>Every additional metre a left lock covers in Speed Zone 2 contributes +0.524 points to the win margin</li>
    </ul>
  </li>
  <li>Athlete Load
    <ul>
      <li>Every additional point in Athlete Load a left lock has contributes -10.9 points to the win margin</li>
    </ul>
  </li>
  <li>Sprints Total (num)
    <ul>
      <li>Every additional sprint a left lock performs in the match contributes +4.00 points to the win margin</li>
    </ul>
  </li>
</ol>

### Position 5: Right lock

```{r backwards stepwise selection 5, cache=TRUE}
pos5data <- fullyCombined[which(fullyCombined$Position == 5), -c(1:4)]
pos5data$`Work Recovery Ratio` <- droplevels(pos5data$`Work Recovery Ratio`)

pos5data <- pos5data[, -c(40, 42)]

corr5 <- cor(pos5data[, -c(13, 40)])
print("Which variables have high correlations with other variables?")
findCorrelation(corr5, cutoff = 0.999)
findCorrelation(corr5, cutoff = 0.9)
findCorrelation(corr5, cutoff = 0.8)
sort(findCorrelation(corr5, cutoff = 0.75))
findCorrelation(corr5, cutoff = 0.7)
findCorrelation(corr5, cutoff = 0.65)
findCorrelation(corr5, cutoff = 0.6)
findCorrelation(corr5, cutoff = 0.55)

# Removing variables that are causing singularities
pos5data <- pos5data[, -c(1:4, 8, 10:13, 15:18, 20, 23, 26, 27, 30:32, 35, 37)]

model1.5 <- regsubsets(margins ~ ., data = pos5data, method = "backward", nvmax = 100)
coef(model1.5, 1:5)
full1.5 <- lm(margins ~ ., data = pos5data)
summary(full1.5)
```

These five models suggest that the most important GPS variables for a right lock are, beginning from the most important:
<ol>
  <li>Sprints Hi-Inten (num)
    <ul>
      <li>Every additional sprint a right lock performs above the high intensity sprint benchmark contributes +3.83 points to the win margin</li>
    </ul>
  </li>
  <li>Sprints Speed Zone 3 (num)
    <ul>
      <li>Every additional sprint a right lock performs in Speed Zone 3 contributes -5.52 points to the win margin</li>
    </ul>
  </li>
  <li>HIE Rate
    <ul>
      <li>For every additional point in a right lock's HIE Rate, -115 points(!) are added to the win margin</li>
    </ul>
  </li>
  <li>Speed Max (km/h)
    <ul>
      <li>For every kilometre per hour in a right lock's maximum speed in a match, +3.14 points are added to the win margin</li>
    </ul>
  </li>
  <li>Distance HR Hi-Inten (m)
    <ul>
      <li>Every additional metre a right lock covers above the high intensity heart rate benchmark contributes -0.0417 points to the win margin</li>
    </ul>
  </li>
</ol>

### Position 6: Blindside flanker

```{r backwards stepwise selection 6, cache=TRUE}
pos6data <- fullyCombined[which(fullyCombined$Position == 6), -c(1:4)]
pos6data$`Work Recovery Ratio` <- droplevels(pos6data$`Work Recovery Ratio`)
# All zeroes in Duration Speed Hi-Inten (s)
pos6data <- pos6data[, -c(2, 40, 42)]

corr6 <- cor(pos6data[, -c(12, 39)])
print("Which variables have high correlations with other variables?")
findCorrelation(corr6, cutoff = 0.999)
findCorrelation(corr6, cutoff = 0.9)
findCorrelation(corr6, cutoff = 0.85)
findCorrelation(corr6, cutoff = 0.8)

# Removing variables that are causing singularities
pos6data <- pos6data[, -c(3, 5:7, 9, 10, 14, 16, 18, 19, 22, 27)]

model1.6 <- regsubsets(margins ~ ., data = pos6data, method = "backward", nvmax = 100)
coef(model1.6, 1:5)
full1.6 <- lm(margins ~ ., data = pos6data)
summary(full1.6)
```

These five models suggest that the most important GPS variables for a blindside flanker are, beginning from the most important:
<ol>
  <li>Distance Speed Zone 2 (m)
    <ul>
      <li>Every additional metre a blindside flanker covers in Speed Zone 2 contributes +0.212 points to the win margin</li>
    </ul>
  </li>
  <li>Distance Speed Zone 3 (m)
    <ul>
      <li>Every additional metre a blindside flanker covers in Speed Zone 3 contributes -0.171 points to the win margin</li>
    </ul>
  </li>
  <li>Decelerations Zone 3 (num)
    <ul>
      <li>Every additional deceleration a blindside flanker performs in Deceleration Zone 3 contributes -4.98 points to the win margin</li>
    </ul>
  </li>
  <li>% Max HR
    <ul>
      <li>For every percentage point of a blindside flanker's match maximum heart rate relative to their maximum heart rate benchmark, -0.582 points are added to the win margin</li>
    </ul>
  </li>
  <li>Body Impacts Grade 1 (num)
    <ul>
      <li>Every additional Grade 1 body impact a blindside flanker performs contributes -3.21 points to the win margin</li>
    </ul>
  </li>
</ol>

### Position 7: Openside flanker

